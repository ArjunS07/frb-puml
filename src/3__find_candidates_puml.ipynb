{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tns_name</th>\n",
       "      <th>previous_name</th>\n",
       "      <th>repeater_name</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>gl</th>\n",
       "      <th>gb</th>\n",
       "      <th>exp_up</th>\n",
       "      <th>exp_low</th>\n",
       "      <th>bonsai_snr</th>\n",
       "      <th>bonsai_dm</th>\n",
       "      <th>snr_fitb</th>\n",
       "      <th>dm_fitb</th>\n",
       "      <th>dm_exc_ne2001</th>\n",
       "      <th>dm_exc_ymw16</th>\n",
       "      <th>bc_width</th>\n",
       "      <th>scat_time</th>\n",
       "      <th>flux</th>\n",
       "      <th>fluence</th>\n",
       "      <th>sub_num</th>\n",
       "      <th>width_fitb</th>\n",
       "      <th>sp_idx</th>\n",
       "      <th>sp_run</th>\n",
       "      <th>high_freq</th>\n",
       "      <th>low_freq</th>\n",
       "      <th>peak_freq</th>\n",
       "      <th>chi_sq</th>\n",
       "      <th>dof</th>\n",
       "      <th>flag_frac</th>\n",
       "      <th>excluded_flag</th>\n",
       "      <th>previous_rp_name</th>\n",
       "      <th>is_repeater</th>\n",
       "      <th>redshift</th>\n",
       "      <th>fre_width</th>\n",
       "      <th>fre_width_ob</th>\n",
       "      <th>in_duration</th>\n",
       "      <th>energy</th>\n",
       "      <th>luminosity</th>\n",
       "      <th>T_B</th>\n",
       "      <th>log_dm_fitb</th>\n",
       "      <th>log_bonsai_dm</th>\n",
       "      <th>log_dm_exc_ne2001</th>\n",
       "      <th>log_dm_exc_ymw16</th>\n",
       "      <th>log_bc_width</th>\n",
       "      <th>log_scat_time</th>\n",
       "      <th>log_flux</th>\n",
       "      <th>log_fluence</th>\n",
       "      <th>log_width_fitb</th>\n",
       "      <th>log_high_freq</th>\n",
       "      <th>log_low_freq</th>\n",
       "      <th>log_peak_freq</th>\n",
       "      <th>log_fre_width</th>\n",
       "      <th>log_redshift</th>\n",
       "      <th>log_in_duration</th>\n",
       "      <th>log_energy</th>\n",
       "      <th>log_luminosity</th>\n",
       "      <th>log_T_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FRB20180725A</td>\n",
       "      <td>180725.J0613+67</td>\n",
       "      <td>-9999</td>\n",
       "      <td>93.42</td>\n",
       "      <td>67.07</td>\n",
       "      <td>147.29</td>\n",
       "      <td>21.29</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>716.6</td>\n",
       "      <td>33.2</td>\n",
       "      <td>715.80930</td>\n",
       "      <td>644.2</td>\n",
       "      <td>635.4</td>\n",
       "      <td>0.00295</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.70</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>38.20</td>\n",
       "      <td>-45.80</td>\n",
       "      <td>760.1</td>\n",
       "      <td>485.3</td>\n",
       "      <td>607.4</td>\n",
       "      <td>371857.954</td>\n",
       "      <td>371481</td>\n",
       "      <td>0.403</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.640740</td>\n",
       "      <td>450.875425</td>\n",
       "      <td>274.8</td>\n",
       "      <td>0.180406</td>\n",
       "      <td>2.827944e+40</td>\n",
       "      <td>1.923870e+43</td>\n",
       "      <td>5.515622e+35</td>\n",
       "      <td>2.854797</td>\n",
       "      <td>2.855277</td>\n",
       "      <td>2.809021</td>\n",
       "      <td>2.803047</td>\n",
       "      <td>-2.530178</td>\n",
       "      <td>-2.958607</td>\n",
       "      <td>0.230449</td>\n",
       "      <td>0.612784</td>\n",
       "      <td>-3.528708</td>\n",
       "      <td>2.880871</td>\n",
       "      <td>2.686010</td>\n",
       "      <td>2.783475</td>\n",
       "      <td>2.654057</td>\n",
       "      <td>-0.193318</td>\n",
       "      <td>-0.743748</td>\n",
       "      <td>40.451471</td>\n",
       "      <td>43.284176</td>\n",
       "      <td>35.741595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRB20180727A</td>\n",
       "      <td>180727.J1311+26</td>\n",
       "      <td>-9999</td>\n",
       "      <td>197.72</td>\n",
       "      <td>26.42</td>\n",
       "      <td>24.76</td>\n",
       "      <td>85.60</td>\n",
       "      <td>10.4</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>642.1</td>\n",
       "      <td>12.2</td>\n",
       "      <td>642.13400</td>\n",
       "      <td>620.9</td>\n",
       "      <td>622.4</td>\n",
       "      <td>0.00295</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.58</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-9.20</td>\n",
       "      <td>800.2</td>\n",
       "      <td>400.2</td>\n",
       "      <td>493.3</td>\n",
       "      <td>382969.318</td>\n",
       "      <td>381818</td>\n",
       "      <td>0.387</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.614818</td>\n",
       "      <td>645.927163</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.860778</td>\n",
       "      <td>1.189571e+40</td>\n",
       "      <td>4.823143e+42</td>\n",
       "      <td>2.622746e+35</td>\n",
       "      <td>2.807626</td>\n",
       "      <td>2.807603</td>\n",
       "      <td>2.793022</td>\n",
       "      <td>2.794070</td>\n",
       "      <td>-2.530178</td>\n",
       "      <td>-2.769551</td>\n",
       "      <td>-0.236572</td>\n",
       "      <td>0.363612</td>\n",
       "      <td>-2.856985</td>\n",
       "      <td>2.903199</td>\n",
       "      <td>2.602277</td>\n",
       "      <td>2.693111</td>\n",
       "      <td>2.810184</td>\n",
       "      <td>-0.211253</td>\n",
       "      <td>-0.065109</td>\n",
       "      <td>40.075391</td>\n",
       "      <td>42.683330</td>\n",
       "      <td>35.418756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FRB20180729A</td>\n",
       "      <td>180729.J1316+55</td>\n",
       "      <td>-9999</td>\n",
       "      <td>199.40</td>\n",
       "      <td>55.58</td>\n",
       "      <td>115.26</td>\n",
       "      <td>61.16</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>108.4</td>\n",
       "      <td>206.6</td>\n",
       "      <td>109.59418</td>\n",
       "      <td>78.8</td>\n",
       "      <td>86.8</td>\n",
       "      <td>0.00098</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>11.70</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>16.46</td>\n",
       "      <td>-30.21</td>\n",
       "      <td>692.7</td>\n",
       "      <td>400.2</td>\n",
       "      <td>525.6</td>\n",
       "      <td>264732.041</td>\n",
       "      <td>186953</td>\n",
       "      <td>0.399</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>293.157605</td>\n",
       "      <td>292.5</td>\n",
       "      <td>0.099776</td>\n",
       "      <td>1.070358e+36</td>\n",
       "      <td>7.383140e+38</td>\n",
       "      <td>4.845901e+32</td>\n",
       "      <td>2.039787</td>\n",
       "      <td>2.035029</td>\n",
       "      <td>1.896526</td>\n",
       "      <td>1.938520</td>\n",
       "      <td>-3.008774</td>\n",
       "      <td>-3.802995</td>\n",
       "      <td>1.068186</td>\n",
       "      <td>1.230449</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>2.840545</td>\n",
       "      <td>2.602277</td>\n",
       "      <td>2.720655</td>\n",
       "      <td>2.467101</td>\n",
       "      <td>-2.648161</td>\n",
       "      <td>-1.000975</td>\n",
       "      <td>36.029529</td>\n",
       "      <td>38.868241</td>\n",
       "      <td>32.685375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FRB20180729B</td>\n",
       "      <td>180729.J0558+56</td>\n",
       "      <td>-9999</td>\n",
       "      <td>89.93</td>\n",
       "      <td>56.50</td>\n",
       "      <td>156.90</td>\n",
       "      <td>15.68</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>318.6</td>\n",
       "      <td>22.0</td>\n",
       "      <td>317.22350</td>\n",
       "      <td>223.2</td>\n",
       "      <td>198.8</td>\n",
       "      <td>0.00197</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>14.50</td>\n",
       "      <td>-14.60</td>\n",
       "      <td>800.2</td>\n",
       "      <td>441.8</td>\n",
       "      <td>657.5</td>\n",
       "      <td>425139.488</td>\n",
       "      <td>421337</td>\n",
       "      <td>0.323</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157566</td>\n",
       "      <td>414.871625</td>\n",
       "      <td>358.4</td>\n",
       "      <td>0.271259</td>\n",
       "      <td>4.966122e+38</td>\n",
       "      <td>4.407270e+41</td>\n",
       "      <td>3.166148e+34</td>\n",
       "      <td>2.501365</td>\n",
       "      <td>2.503246</td>\n",
       "      <td>2.348694</td>\n",
       "      <td>2.298416</td>\n",
       "      <td>-2.705534</td>\n",
       "      <td>-3.180456</td>\n",
       "      <td>-0.036212</td>\n",
       "      <td>0.079181</td>\n",
       "      <td>-3.503070</td>\n",
       "      <td>2.903199</td>\n",
       "      <td>2.645226</td>\n",
       "      <td>2.817896</td>\n",
       "      <td>2.617914</td>\n",
       "      <td>-0.802538</td>\n",
       "      <td>-0.566616</td>\n",
       "      <td>38.696017</td>\n",
       "      <td>41.644170</td>\n",
       "      <td>34.500531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FRB20180730A</td>\n",
       "      <td>180730.J0353+87</td>\n",
       "      <td>-9999</td>\n",
       "      <td>57.39</td>\n",
       "      <td>87.19</td>\n",
       "      <td>125.11</td>\n",
       "      <td>25.11</td>\n",
       "      <td>270.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>849.2</td>\n",
       "      <td>89.8</td>\n",
       "      <td>848.90410</td>\n",
       "      <td>789.7</td>\n",
       "      <td>790.5</td>\n",
       "      <td>0.00492</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>5.20</td>\n",
       "      <td>27.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>4.27</td>\n",
       "      <td>-11.31</td>\n",
       "      <td>759.2</td>\n",
       "      <td>400.2</td>\n",
       "      <td>483.5</td>\n",
       "      <td>429165.844</td>\n",
       "      <td>417689</td>\n",
       "      <td>0.329</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.802405</td>\n",
       "      <td>647.063272</td>\n",
       "      <td>359.0</td>\n",
       "      <td>0.259653</td>\n",
       "      <td>2.335510e+41</td>\n",
       "      <td>8.107252e+43</td>\n",
       "      <td>1.508095e+36</td>\n",
       "      <td>2.928859</td>\n",
       "      <td>2.929010</td>\n",
       "      <td>2.897462</td>\n",
       "      <td>2.897902</td>\n",
       "      <td>-2.308035</td>\n",
       "      <td>-2.683401</td>\n",
       "      <td>0.716003</td>\n",
       "      <td>1.431364</td>\n",
       "      <td>-3.329754</td>\n",
       "      <td>2.880356</td>\n",
       "      <td>2.602277</td>\n",
       "      <td>2.684396</td>\n",
       "      <td>2.810947</td>\n",
       "      <td>-0.095607</td>\n",
       "      <td>-0.585606</td>\n",
       "      <td>41.368382</td>\n",
       "      <td>43.908874</td>\n",
       "      <td>36.178429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>FRB20190701A</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>277.47</td>\n",
       "      <td>59.04</td>\n",
       "      <td>88.29</td>\n",
       "      <td>25.72</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>635.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>637.09340</td>\n",
       "      <td>582.8</td>\n",
       "      <td>587.8</td>\n",
       "      <td>0.00197</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>3.30</td>\n",
       "      <td>800.2</td>\n",
       "      <td>400.2</td>\n",
       "      <td>800.2</td>\n",
       "      <td>341779.300</td>\n",
       "      <td>341690</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.572362</td>\n",
       "      <td>628.944866</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.386679</td>\n",
       "      <td>1.226913e+40</td>\n",
       "      <td>1.429842e+43</td>\n",
       "      <td>4.195023e+35</td>\n",
       "      <td>2.804203</td>\n",
       "      <td>2.803252</td>\n",
       "      <td>2.765520</td>\n",
       "      <td>2.769230</td>\n",
       "      <td>-2.705534</td>\n",
       "      <td>-3.142668</td>\n",
       "      <td>0.100371</td>\n",
       "      <td>0.230449</td>\n",
       "      <td>-3.216096</td>\n",
       "      <td>2.903199</td>\n",
       "      <td>2.602277</td>\n",
       "      <td>2.903199</td>\n",
       "      <td>2.798613</td>\n",
       "      <td>-0.242329</td>\n",
       "      <td>-0.412649</td>\n",
       "      <td>40.088814</td>\n",
       "      <td>43.155288</td>\n",
       "      <td>35.622734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>FRB20190701B</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>302.93</td>\n",
       "      <td>80.18</td>\n",
       "      <td>112.88</td>\n",
       "      <td>23.40</td>\n",
       "      <td>69.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>748.9</td>\n",
       "      <td>17.5</td>\n",
       "      <td>749.11400</td>\n",
       "      <td>687.6</td>\n",
       "      <td>688.1</td>\n",
       "      <td>0.00295</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>3.90</td>\n",
       "      <td>-11.80</td>\n",
       "      <td>732.8</td>\n",
       "      <td>400.2</td>\n",
       "      <td>471.5</td>\n",
       "      <td>329229.311</td>\n",
       "      <td>330137</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688973</td>\n",
       "      <td>561.752466</td>\n",
       "      <td>332.6</td>\n",
       "      <td>0.373008</td>\n",
       "      <td>1.178856e+40</td>\n",
       "      <td>1.152717e+43</td>\n",
       "      <td>6.863375e+35</td>\n",
       "      <td>2.874548</td>\n",
       "      <td>2.874424</td>\n",
       "      <td>2.837336</td>\n",
       "      <td>2.837652</td>\n",
       "      <td>-2.530178</td>\n",
       "      <td>-3.468521</td>\n",
       "      <td>0.041393</td>\n",
       "      <td>0.278754</td>\n",
       "      <td>-3.200659</td>\n",
       "      <td>2.864985</td>\n",
       "      <td>2.602277</td>\n",
       "      <td>2.673482</td>\n",
       "      <td>2.749545</td>\n",
       "      <td>-0.161798</td>\n",
       "      <td>-0.428282</td>\n",
       "      <td>40.071461</td>\n",
       "      <td>43.061723</td>\n",
       "      <td>35.836538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>FRB20190701C</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>96.36</td>\n",
       "      <td>81.63</td>\n",
       "      <td>132.18</td>\n",
       "      <td>25.88</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>972.1</td>\n",
       "      <td>16.8</td>\n",
       "      <td>974.19500</td>\n",
       "      <td>915.8</td>\n",
       "      <td>916.6</td>\n",
       "      <td>0.00197</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>46.20</td>\n",
       "      <td>-211.00</td>\n",
       "      <td>495.5</td>\n",
       "      <td>402.2</td>\n",
       "      <td>446.4</td>\n",
       "      <td>285697.192</td>\n",
       "      <td>286362</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943004</td>\n",
       "      <td>181.282298</td>\n",
       "      <td>93.3</td>\n",
       "      <td>0.741120</td>\n",
       "      <td>2.752633e+40</td>\n",
       "      <td>1.882629e+43</td>\n",
       "      <td>2.574619e+36</td>\n",
       "      <td>2.988646</td>\n",
       "      <td>2.987711</td>\n",
       "      <td>2.961801</td>\n",
       "      <td>2.962180</td>\n",
       "      <td>-2.705534</td>\n",
       "      <td>-2.744727</td>\n",
       "      <td>-0.055517</td>\n",
       "      <td>0.397940</td>\n",
       "      <td>-2.841638</td>\n",
       "      <td>2.695044</td>\n",
       "      <td>2.604442</td>\n",
       "      <td>2.649724</td>\n",
       "      <td>2.258355</td>\n",
       "      <td>-0.025486</td>\n",
       "      <td>-0.130111</td>\n",
       "      <td>40.439748</td>\n",
       "      <td>43.274765</td>\n",
       "      <td>36.410713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>FRB20190701D</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>112.10</td>\n",
       "      <td>66.70</td>\n",
       "      <td>149.28</td>\n",
       "      <td>28.38</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>34.4</td>\n",
       "      <td>934.9</td>\n",
       "      <td>44.8</td>\n",
       "      <td>933.36290</td>\n",
       "      <td>877.4</td>\n",
       "      <td>879.4</td>\n",
       "      <td>0.00885</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>1.33</td>\n",
       "      <td>8.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>6.49</td>\n",
       "      <td>-20.90</td>\n",
       "      <td>651.8</td>\n",
       "      <td>400.2</td>\n",
       "      <td>467.6</td>\n",
       "      <td>358566.724</td>\n",
       "      <td>354457</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.900089</td>\n",
       "      <td>478.062518</td>\n",
       "      <td>251.6</td>\n",
       "      <td>0.736807</td>\n",
       "      <td>9.045763e+40</td>\n",
       "      <td>2.658107e+43</td>\n",
       "      <td>1.602564e+35</td>\n",
       "      <td>2.970051</td>\n",
       "      <td>2.970765</td>\n",
       "      <td>2.943198</td>\n",
       "      <td>2.944186</td>\n",
       "      <td>-2.053057</td>\n",
       "      <td>-2.815309</td>\n",
       "      <td>0.123852</td>\n",
       "      <td>0.934498</td>\n",
       "      <td>-2.853872</td>\n",
       "      <td>2.814114</td>\n",
       "      <td>2.602277</td>\n",
       "      <td>2.669875</td>\n",
       "      <td>2.679485</td>\n",
       "      <td>-0.045714</td>\n",
       "      <td>-0.132646</td>\n",
       "      <td>40.956445</td>\n",
       "      <td>43.424572</td>\n",
       "      <td>35.204815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>FRB20190701E</td>\n",
       "      <td>-9999</td>\n",
       "      <td>-9999</td>\n",
       "      <td>138.57</td>\n",
       "      <td>61.71</td>\n",
       "      <td>153.27</td>\n",
       "      <td>40.37</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>888.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>890.47710</td>\n",
       "      <td>848.1</td>\n",
       "      <td>857.0</td>\n",
       "      <td>0.00295</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-5.10</td>\n",
       "      <td>800.2</td>\n",
       "      <td>400.2</td>\n",
       "      <td>410.3</td>\n",
       "      <td>359241.191</td>\n",
       "      <td>356889</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.867410</td>\n",
       "      <td>746.963883</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.224910</td>\n",
       "      <td>1.715152e+40</td>\n",
       "      <td>1.088983e+43</td>\n",
       "      <td>8.899385e+35</td>\n",
       "      <td>2.949623</td>\n",
       "      <td>2.948413</td>\n",
       "      <td>2.928447</td>\n",
       "      <td>2.932981</td>\n",
       "      <td>-2.530178</td>\n",
       "      <td>-3.173925</td>\n",
       "      <td>-0.167491</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>-3.376751</td>\n",
       "      <td>2.903199</td>\n",
       "      <td>2.602277</td>\n",
       "      <td>2.613102</td>\n",
       "      <td>2.873300</td>\n",
       "      <td>-0.061776</td>\n",
       "      <td>-0.647990</td>\n",
       "      <td>40.234303</td>\n",
       "      <td>43.037021</td>\n",
       "      <td>35.949360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>594 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tns_name    previous_name repeater_name      ra    dec      gl  \\\n",
       "0    FRB20180725A  180725.J0613+67         -9999   93.42  67.07  147.29   \n",
       "1    FRB20180727A  180727.J1311+26         -9999  197.72  26.42   24.76   \n",
       "2    FRB20180729A  180729.J1316+55         -9999  199.40  55.58  115.26   \n",
       "3    FRB20180729B  180729.J0558+56         -9999   89.93  56.50  156.90   \n",
       "4    FRB20180730A  180730.J0353+87         -9999   57.39  87.19  125.11   \n",
       "..            ...              ...           ...     ...    ...     ...   \n",
       "589  FRB20190701A            -9999         -9999  277.47  59.04   88.29   \n",
       "590  FRB20190701B            -9999         -9999  302.93  80.18  112.88   \n",
       "591  FRB20190701C            -9999         -9999   96.36  81.63  132.18   \n",
       "592  FRB20190701D            -9999         -9999  112.10  66.70  149.28   \n",
       "593  FRB20190701E            -9999         -9999  138.57  61.71  153.27   \n",
       "\n",
       "        gb  exp_up  exp_low  bonsai_snr  bonsai_dm  snr_fitb    dm_fitb  \\\n",
       "0    21.29    30.0  -9999.0        19.2      716.6      33.2  715.80930   \n",
       "1    85.60    10.4  -9999.0        10.4      642.1      12.2  642.13400   \n",
       "2    61.16    21.0  -9999.0        32.0      108.4     206.6  109.59418   \n",
       "3    15.68    21.0  -9999.0        12.4      318.6      22.0  317.22350   \n",
       "4    25.11   270.0    214.0        69.5      849.2      89.8  848.90410   \n",
       "..     ...     ...      ...         ...        ...       ...        ...   \n",
       "589  25.72    23.0  -9999.0        12.1      635.7      14.6  637.09340   \n",
       "590  23.40    69.0     70.0        15.0      748.9      17.5  749.11400   \n",
       "591  25.88    82.0     82.0        11.5      972.1      16.8  974.19500   \n",
       "592  28.38    34.0  -9999.0        34.4      934.9      44.8  933.36290   \n",
       "593  40.37    15.0  -9999.0        15.2      888.0      16.7  890.47710   \n",
       "\n",
       "     dm_exc_ne2001  dm_exc_ymw16  bc_width  scat_time   flux  fluence  \\\n",
       "0            644.2         635.4   0.00295   0.001100   1.70     4.10   \n",
       "1            620.9         622.4   0.00295   0.001700   0.58     2.31   \n",
       "2             78.8          86.8   0.00098   0.000157  11.70    17.00   \n",
       "3            223.2         198.8   0.00197   0.000660   0.92     1.20   \n",
       "4            789.7         790.5   0.00492   0.002073   5.20    27.00   \n",
       "..             ...           ...       ...        ...    ...      ...   \n",
       "589          582.8         587.8   0.00197   0.000720   1.26     1.70   \n",
       "590          687.6         688.1   0.00295   0.000340   1.10     1.90   \n",
       "591          915.8         916.6   0.00197   0.001800   0.88     2.50   \n",
       "592          877.4         879.4   0.00885   0.001530   1.33     8.60   \n",
       "593          848.1         857.0   0.00295   0.000670   0.68     2.00   \n",
       "\n",
       "     sub_num  width_fitb  sp_idx  sp_run  high_freq  low_freq  peak_freq  \\\n",
       "0          0    0.000296   38.20  -45.80      760.1     485.3      607.4   \n",
       "1          0    0.001390    3.80   -9.20      800.2     400.2      493.3   \n",
       "2          0    0.000100   16.46  -30.21      692.7     400.2      525.6   \n",
       "3          0    0.000314   14.50  -14.60      800.2     441.8      657.5   \n",
       "4          0    0.000468    4.27  -11.31      759.2     400.2      483.5   \n",
       "..       ...         ...     ...     ...        ...       ...        ...   \n",
       "589        0    0.000608   -1.10    3.30      800.2     400.2      800.2   \n",
       "590        0    0.000630    3.90  -11.80      732.8     400.2      471.5   \n",
       "591        0    0.001440   46.20 -211.00      495.5     402.2      446.4   \n",
       "592        0    0.001400    6.49  -20.90      651.8     400.2      467.6   \n",
       "593        0    0.000420    0.30   -5.10      800.2     400.2      410.3   \n",
       "\n",
       "         chi_sq     dof  flag_frac  excluded_flag previous_rp_name  \\\n",
       "0    371857.954  371481      0.403              1            -9999   \n",
       "1    382969.318  381818      0.387              1            -9999   \n",
       "2    264732.041  186953      0.399              1            -9999   \n",
       "3    425139.488  421337      0.323              1            -9999   \n",
       "4    429165.844  417689      0.329              1            -9999   \n",
       "..          ...     ...        ...            ...              ...   \n",
       "589  341779.300  341690      0.451              0            -9999   \n",
       "590  329229.311  330137      0.470              0            -9999   \n",
       "591  285697.192  286362      0.540              0            -9999   \n",
       "592  358566.724  354457      0.431              0            -9999   \n",
       "593  359241.191  356889      0.427              0            -9999   \n",
       "\n",
       "     is_repeater  redshift   fre_width  fre_width_ob  in_duration  \\\n",
       "0              0  0.640740  450.875425         274.8     0.180406   \n",
       "1              0  0.614818  645.927163         400.0     0.860778   \n",
       "2              0  0.002248  293.157605         292.5     0.099776   \n",
       "3              0  0.157566  414.871625         358.4     0.271259   \n",
       "4              0  0.802405  647.063272         359.0     0.259653   \n",
       "..           ...       ...         ...           ...          ...   \n",
       "589            0  0.572362  628.944866         400.0     0.386679   \n",
       "590            0  0.688973  561.752466         332.6     0.373008   \n",
       "591            0  0.943004  181.282298          93.3     0.741120   \n",
       "592            0  0.900089  478.062518         251.6     0.736807   \n",
       "593            0  0.867410  746.963883         400.0     0.224910   \n",
       "\n",
       "           energy    luminosity           T_B  log_dm_fitb  log_bonsai_dm  \\\n",
       "0    2.827944e+40  1.923870e+43  5.515622e+35     2.854797       2.855277   \n",
       "1    1.189571e+40  4.823143e+42  2.622746e+35     2.807626       2.807603   \n",
       "2    1.070358e+36  7.383140e+38  4.845901e+32     2.039787       2.035029   \n",
       "3    4.966122e+38  4.407270e+41  3.166148e+34     2.501365       2.503246   \n",
       "4    2.335510e+41  8.107252e+43  1.508095e+36     2.928859       2.929010   \n",
       "..            ...           ...           ...          ...            ...   \n",
       "589  1.226913e+40  1.429842e+43  4.195023e+35     2.804203       2.803252   \n",
       "590  1.178856e+40  1.152717e+43  6.863375e+35     2.874548       2.874424   \n",
       "591  2.752633e+40  1.882629e+43  2.574619e+36     2.988646       2.987711   \n",
       "592  9.045763e+40  2.658107e+43  1.602564e+35     2.970051       2.970765   \n",
       "593  1.715152e+40  1.088983e+43  8.899385e+35     2.949623       2.948413   \n",
       "\n",
       "     log_dm_exc_ne2001  log_dm_exc_ymw16  log_bc_width  log_scat_time  \\\n",
       "0             2.809021          2.803047     -2.530178      -2.958607   \n",
       "1             2.793022          2.794070     -2.530178      -2.769551   \n",
       "2             1.896526          1.938520     -3.008774      -3.802995   \n",
       "3             2.348694          2.298416     -2.705534      -3.180456   \n",
       "4             2.897462          2.897902     -2.308035      -2.683401   \n",
       "..                 ...               ...           ...            ...   \n",
       "589           2.765520          2.769230     -2.705534      -3.142668   \n",
       "590           2.837336          2.837652     -2.530178      -3.468521   \n",
       "591           2.961801          2.962180     -2.705534      -2.744727   \n",
       "592           2.943198          2.944186     -2.053057      -2.815309   \n",
       "593           2.928447          2.932981     -2.530178      -3.173925   \n",
       "\n",
       "     log_flux  log_fluence  log_width_fitb  log_high_freq  log_low_freq  \\\n",
       "0    0.230449     0.612784       -3.528708       2.880871      2.686010   \n",
       "1   -0.236572     0.363612       -2.856985       2.903199      2.602277   \n",
       "2    1.068186     1.230449       -4.000000       2.840545      2.602277   \n",
       "3   -0.036212     0.079181       -3.503070       2.903199      2.645226   \n",
       "4    0.716003     1.431364       -3.329754       2.880356      2.602277   \n",
       "..        ...          ...             ...            ...           ...   \n",
       "589  0.100371     0.230449       -3.216096       2.903199      2.602277   \n",
       "590  0.041393     0.278754       -3.200659       2.864985      2.602277   \n",
       "591 -0.055517     0.397940       -2.841638       2.695044      2.604442   \n",
       "592  0.123852     0.934498       -2.853872       2.814114      2.602277   \n",
       "593 -0.167491     0.301030       -3.376751       2.903199      2.602277   \n",
       "\n",
       "     log_peak_freq  log_fre_width  log_redshift  log_in_duration  log_energy  \\\n",
       "0         2.783475       2.654057     -0.193318        -0.743748   40.451471   \n",
       "1         2.693111       2.810184     -0.211253        -0.065109   40.075391   \n",
       "2         2.720655       2.467101     -2.648161        -1.000975   36.029529   \n",
       "3         2.817896       2.617914     -0.802538        -0.566616   38.696017   \n",
       "4         2.684396       2.810947     -0.095607        -0.585606   41.368382   \n",
       "..             ...            ...           ...              ...         ...   \n",
       "589       2.903199       2.798613     -0.242329        -0.412649   40.088814   \n",
       "590       2.673482       2.749545     -0.161798        -0.428282   40.071461   \n",
       "591       2.649724       2.258355     -0.025486        -0.130111   40.439748   \n",
       "592       2.669875       2.679485     -0.045714        -0.132646   40.956445   \n",
       "593       2.613102       2.873300     -0.061776        -0.647990   40.234303   \n",
       "\n",
       "     log_luminosity    log_T_B  \n",
       "0         43.284176  35.741595  \n",
       "1         42.683330  35.418756  \n",
       "2         38.868241  32.685375  \n",
       "3         41.644170  34.500531  \n",
       "4         43.908874  36.178429  \n",
       "..              ...        ...  \n",
       "589       43.155288  35.622734  \n",
       "590       43.061723  35.836538  \n",
       "591       43.274765  36.410713  \n",
       "592       43.424572  35.204815  \n",
       "593       43.037021  35.949360  \n",
       "\n",
       "[594 rows x 57 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/features_extracted/chime_frb_catalog_2021.csv\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "FEATURES = [\n",
    "    \"ra\",\n",
    "    # \"dec\",\n",
    "    \"snr_fitb\",\n",
    "    \"log_dm_exc_ymw16\",\n",
    "    # \"log_dm_exc_ne2001\",\n",
    "    \"log_bc_width\",\n",
    "    \"log_flux\",\n",
    "    \"log_fluence\",\n",
    "    \"sp_idx\",\n",
    "    \"sp_run\",\n",
    "    \"log_in_duration\",\n",
    "    \"log_peak_freq\",\n",
    "    \"log_fre_width\",\n",
    "    \"log_T_B\",\n",
    "    \"log_energy\",\n",
    "    'log_luminosity'\n",
    "]\n",
    "FEATURE_LABELS = [\n",
    "    \"Right Ascension\",\n",
    "    # \"Declination\",  # CHIME reports that source density is high due to the long exposure near the North Celestial Pole, so repeater identification is more difficult at higher declinations than lower ones\n",
    "    \"SNR (fitburst)\",\n",
    "    \"Excess DM (YMW16)\",\n",
    "    # \"Excess DM (NE2001)\",  # We choose YMW16 instead of NE2001\n",
    "    \"Boxcar width\",\n",
    "    \"Flux\",\n",
    "    \"Fluence\",\n",
    "    \"Spectral index\",\n",
    "    \"Spectral running\",\n",
    "    \"Rest-frame width\",\n",
    "    \"Peak frequency\",\n",
    "    \"Frequency width\",\n",
    "    \"Brightness temperature\",\n",
    "    \"Burst energy\",\n",
    "    'Luminosity' # Correlated to burst energy and excess DM\n",
    "]\n",
    "X = df[FEATURES]\n",
    "y = df[\"is_repeater\"]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from utils import get_subburst_preserved_train_test, lee_liu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "X = df[FEATURES]\n",
    "y = df[\"is_repeater\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = get_subburst_preserved_train_test(\n",
    "    original_df=df, X=X, y=y, test_size=0.2\n",
    ")\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PU Classifier Optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from pulearn import ElkanotoPuClassifier, WeightedElkanotoPuClassifier\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As determined in the previous notebook\n",
    "# BASE_CLASSIFIERS = [\"SVM\", \"Logistic Regression\", \"LGBM\"]\n",
    "BASE_CLASSIFIERS = [\"LDA\", \"Logistic Regression\", \"SVM\"]\n",
    "CALIBRATED_CLASSIFIERS = [\"Logistic Regression\", \"LDA\"]\n",
    "NUM_TRIALS = 100\n",
    "optimised_models = []\n",
    "models_info = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "def best_base_classifier(clf_name):\n",
    "    match clf_name:\n",
    "        case \"Decision Tree\":\n",
    "            return DecisionTreeClassifier(\n",
    "                min_samples_split=15,\n",
    "                min_samples_leaf=3,\n",
    "                criterion=\"entropy\",\n",
    "                random_state=RANDOM_SEED,\n",
    "            )\n",
    "        case \"Random Forest\":\n",
    "            return RandomForestClassifier(\n",
    "                n_estimators=218,\n",
    "                min_samples_split=31,\n",
    "                min_samples_leaf=24,\n",
    "                criterion=\"gini\",\n",
    "                random_state=RANDOM_SEED,\n",
    "            )\n",
    "        case \"SVM\":\n",
    "            return SVC(\n",
    "                C=8.471801418819979,\n",
    "                degree=5,\n",
    "                kernel=\"linear\",  # fix to linear so we can access coefficients later\n",
    "                probability=True,\n",
    "                random_state=RANDOM_SEED,\n",
    "            )\n",
    "        case \"AdaBoost\":\n",
    "            return AdaBoostClassifier(\n",
    "                n_estimators=157,\n",
    "                learning_rate=0.546,\n",
    "                algorithm=\"SAMME.R\",\n",
    "                random_state=RANDOM_SEED,\n",
    "            )\n",
    "        case \"LGBM\":\n",
    "            return LGBMClassifier(\n",
    "                n_estimators=480,\n",
    "                learning_rate=0.07799402628373488,\n",
    "                subsample=0.10679258738466368,\n",
    "                colsample_bytree=0.9595824521352425,\n",
    "                random_state=RANDOM_SEED,\n",
    "                verbosity=-1,\n",
    "            )\n",
    "        case \"XGBoost\":\n",
    "            return xgb.XGBClassifier(\n",
    "                n_estimators=426,\n",
    "                eta=0.649963516505147,\n",
    "                gamma=0.05081231511820061,\n",
    "                min_child_weight=9.639798532691014,\n",
    "                max_delta_step=9.378914999779363,\n",
    "                max_leaves=168,\n",
    "                max_bin=123,\n",
    "                subsample=0.5353966969732,\n",
    "                colsample_bytree=0.3887651607578353,\n",
    "                random_state=RANDOM_SEED,\n",
    "            )\n",
    "        case \"Logistic Regression\":\n",
    "            return LogisticRegression(\n",
    "                tol=5.6115164153345e-05,\n",
    "                C=63.512210106407046,\n",
    "                solver=\"liblinear\",\n",
    "                max_iter=162,\n",
    "                random_state=RANDOM_SEED,\n",
    "            )\n",
    "        case \"LDA\":\n",
    "            return LinearDiscriminantAnalysis(\n",
    "                solver=\"lsqr\",\n",
    "                store_covariance=True,\n",
    "                tol=2.0511104188433963e-05\n",
    "            )\n",
    "\n",
    "\n",
    "# def build_optimised_model(cleaned_params, best_clf_option):\n",
    "#     match best_clf_option:\n",
    "#         case \"Decision Tree\":\n",
    "#             return DecisionTreeClassifier(**cleaned_params, random_state=RANDOM_SEED)\n",
    "#         case \"Random Forest\":\n",
    "#             return RandomForestClassifier(**cleaned_params, random_state=RANDOM_SEED)\n",
    "#         case \"SVM\":\n",
    "#             return SVC(**cleaned_params, probability=True, random_state=RANDOM_SEED)\n",
    "#         case \"AdaBoost\":\n",
    "#             return AdaBoostClassifier(**cleaned_params, random_state=RANDOM_SEED)\n",
    "#         case \"LGBM\":\n",
    "#             return LGBMClassifier(\n",
    "#                 **cleaned_params, random_state=RANDOM_SEED, verbosity=-1\n",
    "#             )\n",
    "#         case \"XGBoost\":\n",
    "#             return xgb.XGBClassifier(**cleaned_params, random_state=RANDOM_SEED)\n",
    "#         case \"Logistic Regression\":\n",
    "#             cleaned_params[\"solver\"] = cleaned_params.pop(\"solver__lr\")\n",
    "#             return LogisticRegression(**cleaned_params, random_state=RANDOM_SEED)\n",
    "#         case \"LDA\":\n",
    "#             cleaned_params[\"solver\"] = cleaned_params.pop(\"solver__lda\")\n",
    "#             return LinearDiscriminantAnalysis(**cleaned_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elkanoto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-29 16:09:13,598] A new study created in memory with name: no-name-ffba5fa2-46d1-4b56-a769-2f8619cb6caa\n",
      "[I 2023-07-29 16:09:13,602] Trial 0 finished with value: 3.6511175898931 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.5190609389379257}. Best is trial 0 with value: 3.6511175898931.\n",
      "[I 2023-07-29 16:09:13,605] Trial 1 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7063233020424546}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:13,607] Trial 2 finished with value: 2.4909297052154193 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.778936896513396}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:13,609] Trial 3 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.22838315689740368}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:13,612] Trial 4 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.30386039813862936}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:13,614] Trial 5 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.35645329030558426}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:13,617] Trial 6 finished with value: 4.421768707482993 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.45996410688952816}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:13,752] Trial 7 finished with value: 4.081632653061225 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.2193668865811041}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:13,795] Trial 8 finished with value: 4.178571428571429 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.6658781436815229}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:13,902] Trial 9 finished with value: 4.5 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.408106745617721}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:13,907] Trial 10 finished with value: 3.833673469387755 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.10419802806907191}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:13,978] Trial 11 finished with value: 3.9072356215213353 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.5644067929675778}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,048] Trial 12 finished with value: 4.244897959183674 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.6132388410831646}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,053] Trial 13 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.39656400191670843}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,128] Trial 14 finished with value: 4.421768707482993 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.4686144387761339}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,133] Trial 15 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7088521263762596}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,137] Trial 16 finished with value: 3.234207968901846 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7941551278870125}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,142] Trial 17 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6794910876771018}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,146] Trial 18 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7194334460440865}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,151] Trial 19 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.633077498700376}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,155] Trial 20 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7319842388724567}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,159] Trial 21 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.679251631530912}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,164] Trial 22 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.5928199094903567}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,169] Trial 23 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.710766648296721}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,173] Trial 24 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6504380344521133}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,178] Trial 25 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.5591073456120166}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,183] Trial 26 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7477703761672759}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,187] Trial 27 finished with value: 3.395918367346938 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7984879623293806}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,191] Trial 28 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.692337033882187}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,196] Trial 29 finished with value: 3.6511175898931 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.5265635513172485}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,201] Trial 30 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6261293973710524}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,206] Trial 31 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6821846505837538}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,211] Trial 32 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7607573398707123}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,216] Trial 33 finished with value: 3.6511175898931 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.6753299082241282}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,221] Trial 34 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7519459763751248}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,225] Trial 35 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.704049125297277}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,230] Trial 36 finished with value: 3.141783029001074 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.6516359239853238}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,235] Trial 37 finished with value: 3.7732426303854867 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7689447055544717}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,240] Trial 38 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.5946046748612316}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,245] Trial 39 finished with value: 3.833673469387755 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.7069859443620078}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,250] Trial 40 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6674603901274043}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,255] Trial 41 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6896390309643853}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,261] Trial 42 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7381317847430199}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,266] Trial 43 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6401896183404843}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,271] Trial 44 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7736074333991614}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,313] Trial 45 finished with value: 4.081632653061225 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.6867064969442972}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,318] Trial 46 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7340193310816424}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,323] Trial 47 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6114340573860101}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,366] Trial 48 finished with value: 4.178571428571429 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.6682010909634721}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,371] Trial 49 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7806913648606146}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,376] Trial 50 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7189260225051426}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,380] Trial 51 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6646806996605439}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,385] Trial 52 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6926020744531866}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,390] Trial 53 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6397300181219957}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,395] Trial 54 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7265379043551284}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,400] Trial 55 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.5884858703754046}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,439] Trial 56 finished with value: 3.9304610733182157 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.6917255574041232}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,444] Trial 57 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6195040146229343}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,449] Trial 58 finished with value: 3.833673469387755 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.7124016628988353}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,454] Trial 59 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7551666387023869}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,459] Trial 60 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6521197619873337}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,463] Trial 61 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6978026007784831}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,468] Trial 62 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7355004589317248}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,473] Trial 63 finished with value: 3.395918367346938 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7949643737064404}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,477] Trial 64 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6690835293705437}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,482] Trial 65 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7144322135087463}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,487] Trial 66 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7583849144701212}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,492] Trial 67 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6364151196973111}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,496] Trial 68 finished with value: 3.6511175898931 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.6914746435284789}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,501] Trial 69 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7105023322765909}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,534] Trial 70 finished with value: 2.528211284513805 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.7449336142462009}. Best is trial 1 with value: 4.560738581146745.\n",
      "[I 2023-07-29 16:09:14,539] Trial 71 finished with value: 4.823747680890538 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6733190645078442}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,544] Trial 72 finished with value: 4.823747680890538 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6747885865411827}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,550] Trial 73 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6773895009766977}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,554] Trial 74 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6426176514510871}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,559] Trial 75 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6559008740303875}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,564] Trial 76 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6766144694220703}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,569] Trial 77 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6107193595209716}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,574] Trial 78 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7338658305320276}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,578] Trial 79 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6257330860445193}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,583] Trial 80 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7792402114563595}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,587] Trial 81 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6997056012255836}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,592] Trial 82 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6601765943646799}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,597] Trial 83 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7219831672090904}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,604] Trial 84 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.68037259193468}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,619] Trial 85 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7033817433466736}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,624] Trial 86 finished with value: 3.833673469387755 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.7525909953061866}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,629] Trial 87 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7214866239222041}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,670] Trial 88 finished with value: 3.9304610733182157 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.6837215880604434}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,676] Trial 89 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.666315778533146}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,681] Trial 90 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6466458829083168}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,686] Trial 91 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6921911388941385}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,691] Trial 92 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7431977688830006}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,695] Trial 93 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7043499515631356}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,700] Trial 94 finished with value: 3.7732426303854867 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7665111230856756}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,705] Trial 95 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7246169022625031}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,710] Trial 96 finished with value: 4.560738581146745 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6846525510283499}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,715] Trial 97 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6560747125339996}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,720] Trial 98 finished with value: 4.297959183673469 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.6245537186707052}. Best is trial 71 with value: 4.823747680890538.\n",
      "[I 2023-07-29 16:09:14,725] Trial 99 finished with value: 3.833673469387755 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.7024337256459136}. Best is trial 71 with value: 4.823747680890538.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def elkanoto_objective(trial):\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    base_clf_option = trial.suggest_categorical(\"base_clf_option\", BASE_CLASSIFIERS)\n",
    "    best_classifier = best_base_classifier(base_clf_option)\n",
    "    \n",
    "    # Calibrate the classifier using Platt scaling\n",
    "    if base_clf_option not in CALIBRATED_CLASSIFIERS:\n",
    "        best_classifier = CalibratedClassifierCV(\n",
    "            best_classifier, method=\"sigmoid\", cv=10\n",
    "        )\n",
    "    classifier_obj = ElkanotoPuClassifier(\n",
    "        estimator=best_classifier,\n",
    "        hold_out_ratio=trial.suggest_float(\"hold_out_ratio\", 0.1, 0.8),\n",
    "    )\n",
    "    classifier_obj.fit(X_train_scaled, y_train)\n",
    "    y_pred = classifier_obj.predict(X_val_scaled)\n",
    "    return lee_liu_score(y_known=y_val, y_pred=y_pred)\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=RANDOM_SEED)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(elkanoto_objective, n_trials=NUM_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: best_params={'base_clf_option': 'LDA', 'hold_out_ratio': 0.6733190645078442}\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "best_clf_option = best_params[\"base_clf_option\"]\n",
    "print(f'Best params: {best_params=}')\n",
    "best_classifier = best_base_classifier(best_clf_option)\n",
    "if best_clf_option not in CALIBRATED_CLASSIFIERS:\n",
    "    print('Calibating...')\n",
    "    best_classifier = CalibratedClassifierCV(\n",
    "        best_classifier, method=\"sigmoid\", cv=10\n",
    "    )\n",
    "optimised_elkanoto = ElkanotoPuClassifier(\n",
    "    estimator=best_classifier,\n",
    "    hold_out_ratio=best_params[\"hold_out_ratio\"],\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "optimised_models.append(optimised_elkanoto)\n",
    "models_info.append(\n",
    "    {\"model\": \"Classic Elkanoto\", \"params\": best_params, \"ll_score\": study.best_value}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Elkanoto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-29 16:09:14,735] A new study created in memory with name: no-name-4f2f3445-fe81-4e91-b792-3a104abf3b9b\n",
      "[I 2023-07-29 16:09:14,738] Trial 0 finished with value: 1.7205882352941175 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.5190609389379257}. Best is trial 0 with value: 1.7205882352941175.\n",
      "[I 2023-07-29 16:09:14,741] Trial 1 finished with value: 2.25 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.7063233020424546}. Best is trial 1 with value: 2.25.\n",
      "[I 2023-07-29 16:09:14,743] Trial 2 finished with value: 1.9990507831039388 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.778936896513396}. Best is trial 1 with value: 2.25.\n",
      "[I 2023-07-29 16:09:14,746] Trial 3 finished with value: 1.7727272727272727 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.22838315689740368}. Best is trial 1 with value: 2.25.\n",
      "[I 2023-07-29 16:09:14,749] Trial 4 finished with value: 2.7209302325581395 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.30386039813862936}. Best is trial 4 with value: 2.7209302325581395.\n",
      "[I 2023-07-29 16:09:14,751] Trial 5 finished with value: 2.5999999999999996 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.35645329030558426}. Best is trial 4 with value: 2.7209302325581395.\n",
      "[I 2023-07-29 16:09:14,754] Trial 6 finished with value: 2.5999999999999996 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.45996410688952816}. Best is trial 4 with value: 2.7209302325581395.\n",
      "[I 2023-07-29 16:09:14,919] Trial 7 finished with value: 1.9536178107606677 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.2193668865811041}. Best is trial 4 with value: 2.7209302325581395.\n",
      "[I 2023-07-29 16:09:14,963] Trial 8 finished with value: 0.0 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.6658781436815229}. Best is trial 4 with value: 2.7209302325581395.\n",
      "[I 2023-07-29 16:09:15,072] Trial 9 finished with value: 1.697959183673469 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.408106745617721}. Best is trial 4 with value: 2.7209302325581395.\n",
      "[I 2023-07-29 16:09:15,077] Trial 10 finished with value: 2.4893617021276593 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.10419802806907191}. Best is trial 4 with value: 2.7209302325581395.\n",
      "[I 2023-07-29 16:09:15,083] Trial 11 finished with value: 2.3400000000000003 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.3427956239846353}. Best is trial 4 with value: 2.7209302325581395.\n",
      "[I 2023-07-29 16:09:15,087] Trial 12 finished with value: 2.0892857142857144 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.32024921643274173}. Best is trial 4 with value: 2.7209302325581395.\n",
      "[I 2023-07-29 16:09:15,092] Trial 13 finished with value: 1.6027397260273972 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.5368584690206559}. Best is trial 4 with value: 2.7209302325581395.\n",
      "[I 2023-07-29 16:09:15,097] Trial 14 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3180994271368231}. Best is trial 14 with value: 2.853658536585366.\n",
      "[I 2023-07-29 16:09:15,102] Trial 15 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.281213025771393}. Best is trial 14 with value: 2.853658536585366.\n",
      "[I 2023-07-29 16:09:15,108] Trial 16 finished with value: 2.7209302325581395 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.23275841731728736}. Best is trial 14 with value: 2.853658536585366.\n",
      "[I 2023-07-29 16:09:15,113] Trial 17 finished with value: 2.5999999999999996 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.1201293625110777}. Best is trial 14 with value: 2.853658536585366.\n",
      "[I 2023-07-29 16:09:15,118] Trial 18 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.42015191797211227}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,123] Trial 19 finished with value: 2.7857142857142856 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.4438021649799193}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,129] Trial 20 finished with value: 2.925 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.39211202903447784}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,134] Trial 21 finished with value: 2.925 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.39336987346888524}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,140] Trial 22 finished with value: 2.925 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3960116842988023}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,146] Trial 23 finished with value: 1.7205882352941175 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.5050058702675683}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,153] Trial 24 finished with value: 2.925 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3923804679801614}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,231] Trial 25 finished with value: 0.3537414965986394 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.473806538837367}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,237] Trial 26 finished with value: 1.3928571428571428 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.5643845000283878}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,242] Trial 27 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.42298338764082144}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,248] Trial 28 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.45232728722493876}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,253] Trial 29 finished with value: 1.647887323943662 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.581268550019429}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,323] Trial 30 finished with value: 0.2653061224489795 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.4999572157566557}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,329] Trial 31 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.41568176991831046}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,334] Trial 32 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.4106155505973025}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,340] Trial 33 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.43721710386064533}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,345] Trial 34 finished with value: 2.5434782608695654 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.47896584267835757}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,351] Trial 35 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.35760879905593623}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,356] Trial 36 finished with value: 1.7205882352941175 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.530905094563998}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,361] Trial 37 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.42072984147848136}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,366] Trial 38 finished with value: 2.1666666666666665 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.43691248209856337}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,373] Trial 39 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3537058104569674}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,443] Trial 40 finished with value: 0.3537414965986394 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.4769339135074138}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,449] Trial 41 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3626174859664658}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,455] Trial 42 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3614855168899472}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,462] Trial 43 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.4202567032779683}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,468] Trial 44 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.28866985653997634}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,474] Trial 45 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.37840814420894237}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,479] Trial 46 finished with value: 2.0892857142857144 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.3327143601534625}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,485] Trial 47 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.41675386565305117}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,491] Trial 48 finished with value: 2.925 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.333792991722756}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,638] Trial 49 finished with value: 0.9475218658892127 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.3724657893050289}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,644] Trial 50 finished with value: 2.0172413793103448 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.29872215493252846}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,651] Trial 51 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.44969935990436066}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,657] Trial 52 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.4257545114661129}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,662] Trial 53 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.4203324698757413}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,667] Trial 54 finished with value: 2.925 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3947601445272064}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,672] Trial 55 finished with value: 2.5999999999999996 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.4621428318268238}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,677] Trial 56 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.4094757670250686}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,683] Trial 57 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.35147766153780563}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,688] Trial 58 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3158590127487587}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,694] Trial 59 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.374881506339464}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,698] Trial 60 finished with value: 1.7205882352941175 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.5044246810968402}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,704] Trial 61 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3434181677713797}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,710] Trial 62 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.40309280487607124}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,715] Trial 63 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3807154661739989}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,721] Trial 64 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3563776458137218}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,726] Trial 65 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.436042538399407}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,871] Trial 66 finished with value: 1.697959183673469 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.26853210385642196}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,877] Trial 67 finished with value: 1.746268656716418 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.4661052926137486}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,884] Trial 68 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.4026398030907859}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,891] Trial 69 finished with value: 2.925 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3263519531921268}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,897] Trial 70 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.44596798199595067}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,905] Trial 71 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.35467097015613414}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,911] Trial 72 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.37852967739586263}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,917] Trial 73 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.41738879235274473}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,924] Trial 74 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.36162947998085754}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,930] Trial 75 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3146111997448465}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:15,936] Trial 76 finished with value: 2.5434782608695654 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.49023498804326304}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:16,039] Trial 77 finished with value: 0.7074829931972788 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.39097787247536525}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:16,045] Trial 78 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.43276363317287353}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:16,050] Trial 79 finished with value: 2.5999999999999996 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.4618873255649243}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:16,056] Trial 80 finished with value: 2.1666666666666665 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.3371262055870863}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:16,062] Trial 81 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3638031016302277}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:16,068] Trial 82 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3665130739316317}. Best is trial 18 with value: 3.0.\n",
      "[I 2023-07-29 16:09:16,073] Trial 83 finished with value: 3.0789473684210527 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.38724633067903574}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,079] Trial 84 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.40883425974492343}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,084] Trial 85 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.38372305415694075}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,089] Trial 86 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.4372688486524015}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,095] Trial 87 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.401302158396193}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,101] Trial 88 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.42303207924363884}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,107] Trial 89 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.44774218097657126}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,113] Trial 90 finished with value: 2.925 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3361891369424868}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,119] Trial 91 finished with value: 2.7857142857142856 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3459990399409517}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,125] Trial 92 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3814870109021466}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,131] Trial 93 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3613160200078918}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,137] Trial 94 finished with value: 2.925 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.3930163841205319}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,232] Trial 95 finished with value: 0.9475218658892127 and parameters: {'base_clf_option': 'SVM', 'hold_out_ratio': 0.4205894896629404}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,237] Trial 96 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.4073474191756089}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,243] Trial 97 finished with value: 3.0 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.37086587399382165}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,249] Trial 98 finished with value: 1.7999999999999998 and parameters: {'base_clf_option': 'LDA', 'hold_out_ratio': 0.4571221593343101}. Best is trial 83 with value: 3.0789473684210527.\n",
      "[I 2023-07-29 16:09:16,255] Trial 99 finished with value: 2.853658536585366 and parameters: {'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.31172073989705074}. Best is trial 83 with value: 3.0789473684210527.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def weighted_elkanoto_objective(trial):\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    base_clf_option = trial.suggest_categorical(\"base_clf_option\", BASE_CLASSIFIERS)\n",
    "    best_classifier = best_base_classifier(base_clf_option)\n",
    "    if base_clf_option not in CALIBRATED_CLASSIFIERS:\n",
    "        best_classifier = CalibratedClassifierCV(\n",
    "            best_classifier, method=\"sigmoid\", cv=10\n",
    "        )\n",
    "    classifier_obj = WeightedElkanotoPuClassifier(\n",
    "        estimator=best_classifier,\n",
    "        hold_out_ratio=trial.suggest_float(\"hold_out_ratio\", 0.1, 0.8),\n",
    "        # Cardinality of labeled and unlabeled data\n",
    "        labeled=sum(y_train == 1),\n",
    "        unlabeled=sum(y_train == 0),\n",
    "    )\n",
    "    classifier_obj.fit(X_train_scaled, y_train)\n",
    "    y_pred = classifier_obj.predict(X_val_scaled)\n",
    "    return lee_liu_score(y_known=y_val, y_pred=y_pred)\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=RANDOM_SEED)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(weighted_elkanoto_objective, n_trials=NUM_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params={'base_clf_option': 'Logistic Regression', 'hold_out_ratio': 0.38724633067903574}\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "print(f'{best_params=}')\n",
    "best_clf_option = best_params[\"base_clf_option\"]\n",
    "best_classifier = best_base_classifier(best_clf_option)\n",
    "if best_clf_option not in CALIBRATED_CLASSIFIERS:\n",
    "    print('Calibating...')\n",
    "    best_classifier = CalibratedClassifierCV(\n",
    "        best_classifier, method=\"sigmoid\", cv=10\n",
    "    )\n",
    "optimised_welkanoto = WeightedElkanotoPuClassifier(\n",
    "    estimator=best_classifier,\n",
    "    hold_out_ratio=best_params[\"hold_out_ratio\"],\n",
    "    labeled=sum(y == 1),\n",
    "    unlabeled=sum(y == 0),\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "optimised_models.append(optimised_welkanoto)\n",
    "models_info.append(\n",
    "    {\"model\": \"Weighted Elkanoto\", \"params\": best_params, \"ll_score\": study.best_value}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-29 16:09:16,266] A new study created in memory with name: no-name-0ef58e43-e62f-41a9-8d27-0d5ac0e4b10c\n",
      "[I 2023-07-29 16:09:16,387] Trial 0 finished with value: 3.6836734693877555 and parameters: {'base_clf_option': 'Logistic Regression', 'num_bagged': 130, 'max_samples': 0.24041677639819287, 'max_features': 0.2403950683025824}. Best is trial 0 with value: 3.6836734693877555.\n",
      "[I 2023-07-29 16:09:16,535] Trial 1 finished with value: 3.7741935483870965 and parameters: {'base_clf_option': 'Logistic Regression', 'num_bagged': 149, 'max_samples': 0.1185260448662222, 'max_features': 0.9729188669457949}. Best is trial 1 with value: 3.7741935483870965.\n",
      "[I 2023-07-29 16:09:16,582] Trial 2 finished with value: 3.9906462585034017 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 57, 'max_samples': 0.373818018663584, 'max_features': 0.5722807884690141}. Best is trial 2 with value: 3.9906462585034017.\n",
      "[I 2023-07-29 16:09:16,903] Trial 3 finished with value: 3.831020408163266 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 49, 'max_samples': 0.3629301836816964, 'max_features': 0.4297256589643226}. Best is trial 2 with value: 3.9906462585034017.\n",
      "[I 2023-07-29 16:09:16,999] Trial 4 finished with value: 0.7959183673469388 and parameters: {'base_clf_option': 'Logistic Regression', 'num_bagged': 115, 'max_samples': 0.6331731119758383, 'max_features': 0.14180537144799796}. Best is trial 2 with value: 3.9906462585034017.\n",
      "[I 2023-07-29 16:09:17,179] Trial 5 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 192, 'max_samples': 0.9690688297671034, 'max_features': 0.827557613304815}. Best is trial 5 with value: 4.035445757250268.\n",
      "[I 2023-07-29 16:09:17,722] Trial 6 finished with value: 3.790087463556851 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 102, 'max_samples': 0.20983441136030095, 'max_features': 0.5456592191001431}. Best is trial 5 with value: 4.035445757250268.\n",
      "[I 2023-07-29 16:09:17,857] Trial 7 finished with value: 3.831020408163266 and parameters: {'base_clf_option': 'Logistic Regression', 'num_bagged': 141, 'max_samples': 0.3805399684804699, 'max_features': 0.5680612190600297}. Best is trial 5 with value: 4.035445757250268.\n",
      "[I 2023-07-29 16:09:20,932] Trial 8 finished with value: 3.574650912996777 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 161, 'max_samples': 0.9455490474077702, 'max_features': 0.905344615384884}. Best is trial 5 with value: 4.035445757250268.\n",
      "[I 2023-07-29 16:09:20,983] Trial 9 finished with value: 3.423304805793285 and parameters: {'base_clf_option': 'Logistic Regression', 'num_bagged': 59, 'max_samples': 0.14070456001948428, 'max_features': 0.39279729768693794}. Best is trial 5 with value: 4.035445757250268.\n",
      "[I 2023-07-29 16:09:21,162] Trial 10 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 189, 'max_samples': 0.9725833997090791, 'max_features': 0.7723302704408399}. Best is trial 5 with value: 4.035445757250268.\n",
      "[I 2023-07-29 16:09:21,333] Trial 11 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 184, 'max_samples': 0.9883918994753368, 'max_features': 0.784961492371185}. Best is trial 5 with value: 4.035445757250268.\n",
      "[I 2023-07-29 16:09:21,514] Trial 12 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 200, 'max_samples': 0.8258148137250139, 'max_features': 0.7726547685746881}. Best is trial 5 with value: 4.035445757250268.\n",
      "[I 2023-07-29 16:09:21,671] Trial 13 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 175, 'max_samples': 0.8217638254545288, 'max_features': 0.7791819487952074}. Best is trial 5 with value: 4.035445757250268.\n",
      "[I 2023-07-29 16:09:21,855] Trial 14 finished with value: 3.833673469387755 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 200, 'max_samples': 0.7227480083782317, 'max_features': 0.8698565758169816}. Best is trial 5 with value: 4.035445757250268.\n",
      "[I 2023-07-29 16:09:21,939] Trial 15 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 85, 'max_samples': 0.9972499496121191, 'max_features': 0.6874130802024828}. Best is trial 5 with value: 4.035445757250268.\n",
      "[I 2023-07-29 16:09:22,099] Trial 16 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 164, 'max_samples': 0.8618544592470436, 'max_features': 0.9964148785726921}. Best is trial 5 with value: 4.035445757250268.\n",
      "[I 2023-07-29 16:09:22,261] Trial 17 finished with value: 3.833673469387755 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 182, 'max_samples': 0.7456117277175728, 'max_features': 0.6554427016769668}. Best is trial 5 with value: 4.035445757250268.\n",
      "[I 2023-07-29 16:09:22,347] Trial 18 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 83, 'max_samples': 0.886879503670359, 'max_features': 0.8647819219411235}. Best is trial 5 with value: 4.035445757250268.\n",
      "[I 2023-07-29 16:09:22,455] Trial 19 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 127, 'max_samples': 0.5607921510421621, 'max_features': 0.6792189455463115}. Best is trial 19 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:09:22,788] Trial 20 finished with value: 3.9072356215213353 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 31, 'max_samples': 0.5367791765777158, 'max_features': 0.676027952266316}. Best is trial 19 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:09:22,936] Trial 21 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 158, 'max_samples': 0.9152792876100969, 'max_features': 0.7377799534752892}. Best is trial 19 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:09:23,045] Trial 22 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 115, 'max_samples': 0.7509376184057113, 'max_features': 0.8406264692501865}. Best is trial 19 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:09:23,179] Trial 23 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 134, 'max_samples': 0.924429410577791, 'max_features': 0.943219212425509}. Best is trial 19 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:09:23,329] Trial 24 finished with value: 3.833673469387755 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 175, 'max_samples': 0.6035787696627208, 'max_features': 0.8235974175582936}. Best is trial 19 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:09:23,500] Trial 25 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 190, 'max_samples': 0.7982423974446629, 'max_features': 0.7250959448996233}. Best is trial 19 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:09:23,595] Trial 26 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 94, 'max_samples': 0.8694972785921395, 'max_features': 0.9342855155214208}. Best is trial 19 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:09:23,761] Trial 27 finished with value: 4.035445757250268 and parameters: {'base_clf_option': 'LDA', 'num_bagged': 168, 'max_samples': 0.9994163504324306, 'max_features': 0.8168308200478943}. Best is trial 19 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:09:26,188] Trial 28 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 152, 'max_samples': 0.6934208295750458, 'max_features': 0.9011282639760233}. Best is trial 19 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:09:28,212] Trial 29 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 129, 'max_samples': 0.6803390610021364, 'max_features': 0.8941719827355264}. Best is trial 19 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:09:30,056] Trial 30 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 123, 'max_samples': 0.6581743977401174, 'max_features': 0.8984248947570911}. Best is trial 19 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:09:32,010] Trial 31 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 123, 'max_samples': 0.6710563145645977, 'max_features': 0.9050328059209665}. Best is trial 19 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:09:34,115] Trial 32 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 146, 'max_samples': 0.5629245523483962, 'max_features': 0.9848241626119603}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:09:36,114] Trial 33 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 147, 'max_samples': 0.5366396668575334, 'max_features': 0.9883301329116561}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:09:38,060] Trial 34 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 147, 'max_samples': 0.5366000132654213, 'max_features': 0.9751952414871206}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:09:39,973] Trial 35 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 137, 'max_samples': 0.5160481762663319, 'max_features': 0.9953681012228426}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:09:41,776] Trial 36 finished with value: 3.9906462585034017 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 143, 'max_samples': 0.47790291829725934, 'max_features': 0.9968696853912449}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:09:43,610] Trial 37 finished with value: 3.9906462585034017 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 149, 'max_samples': 0.4750202286443882, 'max_features': 0.9575675773125726}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:09:45,622] Trial 38 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 137, 'max_samples': 0.5927120949660539, 'max_features': 0.997722845632876}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:09:46,955] Trial 39 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 103, 'max_samples': 0.5010151911000454, 'max_features': 0.9589232147027559}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:09:48,805] Trial 40 finished with value: 3.9906462585034017 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 152, 'max_samples': 0.43401262325071666, 'max_features': 0.9378924223750176}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:09:50,874] Trial 41 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 136, 'max_samples': 0.594690888103477, 'max_features': 0.9994721446789168}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:09:52,850] Trial 42 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 139, 'max_samples': 0.5615829920068897, 'max_features': 0.9642533188650928}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:09:55,019] Trial 43 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 144, 'max_samples': 0.6251902018616016, 'max_features': 0.9997445537703095}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:09:55,158] Trial 44 finished with value: 3.831020408163266 and parameters: {'base_clf_option': 'Logistic Regression', 'num_bagged': 115, 'max_samples': 0.419344836760132, 'max_features': 0.9326138209668476}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:09:57,156] Trial 45 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 157, 'max_samples': 0.5057966996056662, 'max_features': 0.8596409543526952}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:09:59,210] Trial 46 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 134, 'max_samples': 0.5847787411684275, 'max_features': 0.9591471075312108}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:10:01,372] Trial 47 finished with value: 3.9072356215213353 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 168, 'max_samples': 0.5281060828145785, 'max_features': 0.9213647117861343}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:10:01,503] Trial 48 finished with value: 3.9072356215213353 and parameters: {'base_clf_option': 'Logistic Regression', 'num_bagged': 105, 'max_samples': 0.6355524194396602, 'max_features': 0.8747557389643}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:10:03,450] Trial 49 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 146, 'max_samples': 0.5497582314735899, 'max_features': 0.9772144259410528}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:10:04,556] Trial 50 finished with value: 4.081632653061225 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 122, 'max_samples': 0.3380978690537726, 'max_features': 0.8354478560942221}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:10:05,925] Trial 51 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 106, 'max_samples': 0.5014071781650229, 'max_features': 0.9668994818857065}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:10:07,303] Trial 52 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 95, 'max_samples': 0.5678268179007198, 'max_features': 0.9355890910285231}. Best is trial 32 with value: 4.164152617568767.\n",
      "[I 2023-07-29 16:10:09,702] Trial 53 finished with value: 4.353432282003711 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 156, 'max_samples': 0.6102120095432617, 'max_features': 0.9770192274652224}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:12,070] Trial 54 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 157, 'max_samples': 0.6445472237474918, 'max_features': 0.9994189147478976}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:14,564] Trial 55 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 173, 'max_samples': 0.6075443987384499, 'max_features': 0.8853303383279476}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:14,750] Trial 56 finished with value: 3.9072356215213353 and parameters: {'base_clf_option': 'Logistic Regression', 'num_bagged': 152, 'max_samples': 0.583472084318743, 'max_features': 0.9178426611217354}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:16,897] Trial 57 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 160, 'max_samples': 0.5466097336701314, 'max_features': 0.9699043748284073}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:18,975] Trial 58 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 141, 'max_samples': 0.6258614216217853, 'max_features': 0.8644123206369655}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:21,085] Trial 59 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 165, 'max_samples': 0.5278716899310725, 'max_features': 0.9177730987534243}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:23,839] Trial 60 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 181, 'max_samples': 0.6040455339493307, 'max_features': 0.96444792127278}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:25,647] Trial 61 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 131, 'max_samples': 0.49396059709366097, 'max_features': 0.9444506884849625}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:26,649] Trial 62 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 71, 'max_samples': 0.5190338984244808, 'max_features': 0.9766064287214123}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:27,972] Trial 63 finished with value: 3.9906462585034017 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 110, 'max_samples': 0.45128013138956624, 'max_features': 0.9112429592988311}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:30,006] Trial 64 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 138, 'max_samples': 0.5691955976487129, 'max_features': 0.9486497722251458}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:31,554] Trial 65 finished with value: 3.9072356215213353 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 119, 'max_samples': 0.5310954497075868, 'max_features': 0.8745854675561267}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:33,106] Trial 66 finished with value: 3.9906462585034017 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 127, 'max_samples': 0.4690338830968023, 'max_features': 0.9752828872012139}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:34,304] Trial 67 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 99, 'max_samples': 0.5009359598628883, 'max_features': 0.8926502438381748}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:34,469] Trial 68 finished with value: 3.737355811889973 and parameters: {'base_clf_option': 'Logistic Regression', 'num_bagged': 147, 'max_samples': 0.5525558452352538, 'max_features': 0.8098149093833181}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:35,807] Trial 69 finished with value: 3.833673469387755 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 83, 'max_samples': 0.7050303331779392, 'max_features': 0.9999083111961521}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:38,038] Trial 70 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 155, 'max_samples': 0.6548882956743941, 'max_features': 0.8485492730141527}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:40,082] Trial 71 finished with value: 4.353432282003711 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 134, 'max_samples': 0.6056504155223692, 'max_features': 0.9970473821526905}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:41,979] Trial 72 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 131, 'max_samples': 0.5819212630532253, 'max_features': 0.9372143539698499}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:44,529] Trial 73 finished with value: 4.353432282003711 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 143, 'max_samples': 0.6075577789310138, 'max_features': 0.9721608053973341}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:46,759] Trial 74 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 149, 'max_samples': 0.6241478226752377, 'max_features': 0.9168952139033655}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:48,991] Trial 75 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 141, 'max_samples': 0.6622835746962754, 'max_features': 0.9809217559517853}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:51,439] Trial 76 finished with value: 4.353432282003711 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 164, 'max_samples': 0.6096705240654843, 'max_features': 0.9545484617927033}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:54,110] Trial 77 finished with value: 4.353432282003711 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 173, 'max_samples': 0.6117111732148411, 'max_features': 0.949569166544931}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:56,875] Trial 78 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 173, 'max_samples': 0.6890983463443185, 'max_features': 0.8913440667858027}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:59,394] Trial 79 finished with value: 4.353432282003711 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 163, 'max_samples': 0.6049028278265927, 'max_features': 0.9286100875563423}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:10:59,601] Trial 80 finished with value: 3.9072356215213353 and parameters: {'base_clf_option': 'Logistic Regression', 'num_bagged': 164, 'max_samples': 0.6141986193155347, 'max_features': 0.9282473429587129}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:02,726] Trial 81 finished with value: 4.353432282003711 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 195, 'max_samples': 0.6587836424087644, 'max_features': 0.946889237324544}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:05,706] Trial 82 finished with value: 4.353432282003711 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 194, 'max_samples': 0.6424869292184856, 'max_features': 0.9486001500145849}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:08,700] Trial 83 finished with value: 4.353432282003711 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 196, 'max_samples': 0.6454842992020339, 'max_features': 0.9529006097502917}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:11,775] Trial 84 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 197, 'max_samples': 0.6749809212065572, 'max_features': 0.9501015717723946}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:14,553] Trial 85 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 185, 'max_samples': 0.6416720624987591, 'max_features': 0.8975828319160478}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:17,660] Trial 86 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 194, 'max_samples': 0.7271485536977425, 'max_features': 0.9464128188851203}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:20,247] Trial 87 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 179, 'max_samples': 0.6572030128785951, 'max_features': 0.8477159931411247}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:23,151] Trial 88 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 187, 'max_samples': 0.7047098562311114, 'max_features': 0.8765726084544271}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:26,106] Trial 89 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 200, 'max_samples': 0.6358481175009313, 'max_features': 0.9167833765768989}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:28,740] Trial 90 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 193, 'max_samples': 0.6124649705208878, 'max_features': 0.801834201569513}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:31,247] Trial 91 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 170, 'max_samples': 0.5984701812407894, 'max_features': 0.9555551282698322}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:34,158] Trial 92 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 188, 'max_samples': 0.6706502394110705, 'max_features': 0.9778256219085608}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:36,622] Trial 93 finished with value: 3.9072356215213353 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 179, 'max_samples': 0.5832395870504796, 'max_features': 0.9271109487472019}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:39,040] Trial 94 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 163, 'max_samples': 0.6472080321895564, 'max_features': 0.9042591012634595}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:41,814] Trial 95 finished with value: 4.164152617568767 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 191, 'max_samples': 0.5669201425962708, 'max_features': 0.954609858047401}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:44,732] Trial 96 finished with value: 4.353432282003711 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 178, 'max_samples': 0.6183054444941413, 'max_features': 0.9353770838312505}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:47,705] Trial 97 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 183, 'max_samples': 0.6290486748970198, 'max_features': 0.883811730533911}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:50,311] Trial 98 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 178, 'max_samples': 0.614779484581156, 'max_features': 0.8605420482631327}. Best is trial 53 with value: 4.353432282003711.\n",
      "[I 2023-07-29 16:11:53,025] Trial 99 finished with value: 4.093294460641399 and parameters: {'base_clf_option': 'SVM', 'num_bagged': 171, 'max_samples': 0.686621292444927, 'max_features': 0.9388656700874025}. Best is trial 53 with value: 4.353432282003711.\n"
     ]
    }
   ],
   "source": [
    "from pulearn import BaggingPuClassifier\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def bagging_svm_objective(trial):\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    base_clf_option = trial.suggest_categorical(\"base_clf_option\", BASE_CLASSIFIERS)\n",
    "    best_classifier = best_base_classifier(base_clf_option)\n",
    "    classifier_obj = BaggingPuClassifier(\n",
    "        base_estimator=best_classifier,\n",
    "        n_estimators=trial.suggest_int(\"num_bagged\", 25, 200),\n",
    "        max_samples=trial.suggest_float(\"max_samples\", 0.1, 1.0),\n",
    "        max_features=trial.suggest_float(\"max_features\", 0.1, 1.0),\n",
    "    )\n",
    "    classifier_obj.fit(X_train_scaled, y_train)\n",
    "    y_pred = classifier_obj.predict(X_val_scaled)\n",
    "    return lee_liu_score(y_known=y_val, y_pred=y_pred)\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=RANDOM_SEED)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(bagging_svm_objective, n_trials=NUM_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params={'base_clf_option': 'SVM', 'num_bagged': 156, 'max_samples': 0.6102120095432617, 'max_features': 0.9770192274652224}\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "print(f'{best_params=}')\n",
    "optimised_bagging = BaggingPuClassifier(\n",
    "    base_estimator=best_base_classifier(best_params['base_clf_option']),\n",
    "    n_estimators=best_params[\"num_bagged\"],\n",
    "    max_samples= best_params[\"max_samples\"],\n",
    "    max_features=best_params[\"max_features\"],\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "optimised_models.append(optimised_bagging)\n",
    "models_info.append(\n",
    "    {\"model\": \"Bagging Classifier\", \"params\": best_params, \"ll_score\": study.best_value}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-29 16:11:53,036] A new study created in memory with name: no-name-aeebd324-0a00-40e5-8ea8-ea7188d67f3c\n",
      "[I 2023-07-29 16:11:55,284] Trial 0 finished with value: 3.833673469387755 and parameters: {'epochs': 437, 'learning_rate': 0.09507635921035062}. Best is trial 0 with value: 3.833673469387755.\n",
      "[I 2023-07-29 16:11:59,224] Trial 1 finished with value: 3.833673469387755 and parameters: {'epochs': 759, 'learning_rate': 0.05990598257128396}. Best is trial 0 with value: 3.833673469387755.\n",
      "[I 2023-07-29 16:12:00,445] Trial 2 finished with value: 4.093294460641399 and parameters: {'epochs': 240, 'learning_rate': 0.015683852581586645}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:01,231] Trial 3 finished with value: 3.574650912996777 and parameters: {'epochs': 152, 'learning_rate': 0.08663099696291603}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:04,534] Trial 4 finished with value: 3.833673469387755 and parameters: {'epochs': 641, 'learning_rate': 0.07083645052182495}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:05,164] Trial 5 finished with value: 3.833673469387755 and parameters: {'epochs': 118, 'learning_rate': 0.09699399423098323}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:09,522] Trial 6 finished with value: 3.833673469387755 and parameters: {'epochs': 850, 'learning_rate': 0.021312677156759788}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:10,839] Trial 7 finished with value: 4.093294460641399 and parameters: {'epochs': 263, 'learning_rate': 0.01842211053435804}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:12,755] Trial 8 finished with value: 3.833673469387755 and parameters: {'epochs': 374, 'learning_rate': 0.05252316752006057}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:15,350] Trial 9 finished with value: 3.833673469387755 and parameters: {'epochs': 489, 'learning_rate': 0.02919379110578439}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:20,316] Trial 10 finished with value: 2.335988053758089 and parameters: {'epochs': 951, 'learning_rate': 0.0006991185772861272}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:21,789] Trial 11 finished with value: 3.833673469387755 and parameters: {'epochs': 294, 'learning_rate': 0.028754146824838272}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:23,121] Trial 12 finished with value: 2.8022959183673466 and parameters: {'epochs': 251, 'learning_rate': 0.003968647476122189}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:26,326] Trial 13 finished with value: 3.833673469387755 and parameters: {'epochs': 593, 'learning_rate': 0.03930604392105299}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:27,690] Trial 14 finished with value: 3.833673469387755 and parameters: {'epochs': 258, 'learning_rate': 0.0126427419074494}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:29,507] Trial 15 finished with value: 4.093294460641399 and parameters: {'epochs': 352, 'learning_rate': 0.01420132747301215}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:30,517] Trial 16 finished with value: 4.093294460641399 and parameters: {'epochs': 194, 'learning_rate': 0.037883069949265415}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:33,136] Trial 17 finished with value: 3.833673469387755 and parameters: {'epochs': 508, 'learning_rate': 0.015132310221312818}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:35,165] Trial 18 finished with value: 3.833673469387755 and parameters: {'epochs': 376, 'learning_rate': 0.04024140646974365}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:38,622] Trial 19 finished with value: 1.0263157894736843 and parameters: {'epochs': 669, 'learning_rate': 0.0008311388829606407}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:39,684] Trial 20 finished with value: 4.093294460641399 and parameters: {'epochs': 202, 'learning_rate': 0.024156712956340923}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:41,425] Trial 21 finished with value: 4.093294460641399 and parameters: {'epochs': 347, 'learning_rate': 0.011696257915306617}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:43,515] Trial 22 finished with value: 3.833673469387755 and parameters: {'epochs': 415, 'learning_rate': 0.017124235523419597}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:45,066] Trial 23 finished with value: 3.833673469387755 and parameters: {'epochs': 304, 'learning_rate': 0.00948672432748655}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:46,110] Trial 24 finished with value: 4.093294460641399 and parameters: {'epochs': 206, 'learning_rate': 0.021499651292489864}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:46,653] Trial 25 finished with value: 2.8022959183673466 and parameters: {'epochs': 105, 'learning_rate': 0.009098784662111042}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:49,010] Trial 26 finished with value: 3.833673469387755 and parameters: {'epochs': 456, 'learning_rate': 0.03068610858089406}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:51,756] Trial 27 finished with value: 3.833673469387755 and parameters: {'epochs': 547, 'learning_rate': 0.016037136740854017}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:53,270] Trial 28 finished with value: 3.574650912996777 and parameters: {'epochs': 300, 'learning_rate': 0.005927252263832538}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:55,479] Trial 29 finished with value: 3.833673469387755 and parameters: {'epochs': 433, 'learning_rate': 0.021121899871053597}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:57,194] Trial 30 finished with value: 3.574650912996777 and parameters: {'epochs': 342, 'learning_rate': 0.006539277895763352}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:58,085] Trial 31 finished with value: 4.093294460641399 and parameters: {'epochs': 176, 'learning_rate': 0.03582986283321925}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:12:59,257] Trial 32 finished with value: 3.833673469387755 and parameters: {'epochs': 230, 'learning_rate': 0.04600322949320854}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:00,005] Trial 33 finished with value: 4.093294460641399 and parameters: {'epochs': 147, 'learning_rate': 0.027424646405115556}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:01,277] Trial 34 finished with value: 3.833673469387755 and parameters: {'epochs': 248, 'learning_rate': 0.03521701364750787}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:02,173] Trial 35 finished with value: 3.833673469387755 and parameters: {'epochs': 177, 'learning_rate': 0.017618742174956348}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:03,753] Trial 36 finished with value: 3.833673469387755 and parameters: {'epochs': 314, 'learning_rate': 0.024571860808414205}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:04,269] Trial 37 finished with value: 4.093294460641399 and parameters: {'epochs': 101, 'learning_rate': 0.0639828997804607}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:06,256] Trial 38 finished with value: 3.833673469387755 and parameters: {'epochs': 397, 'learning_rate': 0.02041277458227983}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:10,003] Trial 39 finished with value: 3.833673469387755 and parameters: {'epochs': 750, 'learning_rate': 0.011800522544270893}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:10,770] Trial 40 finished with value: 4.093294460641399 and parameters: {'epochs': 152, 'learning_rate': 0.032293997742450314}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:11,830] Trial 41 finished with value: 4.093294460641399 and parameters: {'epochs': 202, 'learning_rate': 0.025237397431008633}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:13,196] Trial 42 finished with value: 4.093294460641399 and parameters: {'epochs': 268, 'learning_rate': 0.023063160285863367}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:14,329] Trial 43 finished with value: 4.093294460641399 and parameters: {'epochs': 211, 'learning_rate': 0.026856672056863776}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:15,151] Trial 44 finished with value: 3.833673469387755 and parameters: {'epochs': 156, 'learning_rate': 0.01685396506242081}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:17,037] Trial 45 finished with value: 3.833673469387755 and parameters: {'epochs': 346, 'learning_rate': 0.03336054090361001}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:18,419] Trial 46 finished with value: 3.833673469387755 and parameters: {'epochs': 266, 'learning_rate': 0.0286350777874961}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:19,588] Trial 47 finished with value: 4.093294460641399 and parameters: {'epochs': 224, 'learning_rate': 0.019795554475690355}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:20,283] Trial 48 finished with value: 3.574650912996777 and parameters: {'epochs': 137, 'learning_rate': 0.01160113979651264}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:25,176] Trial 49 finished with value: 3.833673469387755 and parameters: {'epochs': 935, 'learning_rate': 0.0405241965514852}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:27,565] Trial 50 finished with value: 3.574650912996777 and parameters: {'epochs': 461, 'learning_rate': 0.004897469695949161}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:29,424] Trial 51 finished with value: 4.093294460641399 and parameters: {'epochs': 332, 'learning_rate': 0.013809393306270486}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:31,500] Trial 52 finished with value: 4.093294460641399 and parameters: {'epochs': 371, 'learning_rate': 0.01148272566637486}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:32,950] Trial 53 finished with value: 4.093294460641399 and parameters: {'epochs': 272, 'learning_rate': 0.024131454099845715}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:33,913] Trial 54 finished with value: 3.574650912996777 and parameters: {'epochs': 186, 'learning_rate': 0.008108079735940863}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:36,530] Trial 55 finished with value: 1.0 and parameters: {'epochs': 510, 'learning_rate': 0.00035209941155483516}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:37,997] Trial 56 finished with value: 4.093294460641399 and parameters: {'epochs': 286, 'learning_rate': 0.01545604908562796}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:39,924] Trial 57 finished with value: 3.833673469387755 and parameters: {'epochs': 374, 'learning_rate': 0.019965966951562896}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:41,136] Trial 58 finished with value: 3.574650912996777 and parameters: {'epochs': 226, 'learning_rate': 0.009243946698541214}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:42,875] Trial 59 finished with value: 3.316326530612245 and parameters: {'epochs': 333, 'learning_rate': 0.0036188659392803225}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:44,154] Trial 60 finished with value: 3.833673469387755 and parameters: {'epochs': 247, 'learning_rate': 0.030663115299478814}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:45,126] Trial 61 finished with value: 4.093294460641399 and parameters: {'epochs': 190, 'learning_rate': 0.022162164549342164}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:46,751] Trial 62 finished with value: 4.093294460641399 and parameters: {'epochs': 302, 'learning_rate': 0.01837528113533155}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:47,432] Trial 63 finished with value: 3.574650912996777 and parameters: {'epochs': 132, 'learning_rate': 0.014266332715372903}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:48,759] Trial 64 finished with value: 4.093294460641399 and parameters: {'epochs': 237, 'learning_rate': 0.023304072859640614}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:50,978] Trial 65 finished with value: 3.833673469387755 and parameters: {'epochs': 407, 'learning_rate': 0.026111261936432865}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:51,827] Trial 66 finished with value: 3.833673469387755 and parameters: {'epochs': 166, 'learning_rate': 0.018502892073496334}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:53,004] Trial 67 finished with value: 3.833673469387755 and parameters: {'epochs': 217, 'learning_rate': 0.013849079964389197}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:53,704] Trial 68 finished with value: 4.093294460641399 and parameters: {'epochs': 121, 'learning_rate': 0.0289439689655532}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:55,182] Trial 69 finished with value: 3.833673469387755 and parameters: {'epochs': 287, 'learning_rate': 0.009241568547864943}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:56,994] Trial 70 finished with value: 4.093294460641399 and parameters: {'epochs': 357, 'learning_rate': 0.01577625566958194}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:57,997] Trial 71 finished with value: 4.093294460641399 and parameters: {'epochs': 192, 'learning_rate': 0.0347993038925393}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:13:59,326] Trial 72 finished with value: 3.833673469387755 and parameters: {'epochs': 251, 'learning_rate': 0.03765098168818347}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:00,230] Trial 73 finished with value: 4.093294460641399 and parameters: {'epochs': 176, 'learning_rate': 0.021795263898240116}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:03,736] Trial 74 finished with value: 3.833673469387755 and parameters: {'epochs': 674, 'learning_rate': 0.04260594841634893}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:05,438] Trial 75 finished with value: 3.833673469387755 and parameters: {'epochs': 318, 'learning_rate': 0.03202163066293272}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:06,528] Trial 76 finished with value: 4.093294460641399 and parameters: {'epochs': 211, 'learning_rate': 0.025355766080821268}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:07,392] Trial 77 finished with value: 4.093294460641399 and parameters: {'epochs': 169, 'learning_rate': 0.028748695051770377}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:07,919] Trial 78 finished with value: 3.833673469387755 and parameters: {'epochs': 102, 'learning_rate': 0.036527030879118016}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:09,294] Trial 79 finished with value: 4.093294460641399 and parameters: {'epochs': 271, 'learning_rate': 0.018581830309851596}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:10,052] Trial 80 finished with value: 4.093294460641399 and parameters: {'epochs': 149, 'learning_rate': 0.033962901145078064}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:11,099] Trial 81 finished with value: 4.093294460641399 and parameters: {'epochs': 205, 'learning_rate': 0.02735935575782313}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:14,176] Trial 82 finished with value: 3.833673469387755 and parameters: {'epochs': 607, 'learning_rate': 0.02144293193695077}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:14,855] Trial 83 finished with value: 3.833673469387755 and parameters: {'epochs': 133, 'learning_rate': 0.03042312537250085}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:16,056] Trial 84 finished with value: 4.093294460641399 and parameters: {'epochs': 235, 'learning_rate': 0.024287387465741953}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:16,932] Trial 85 finished with value: 3.574650912996777 and parameters: {'epochs': 170, 'learning_rate': 0.011292998310761821}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:17,715] Trial 86 finished with value: 4.093294460641399 and parameters: {'epochs': 151, 'learning_rate': 0.02728503395838251}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:18,731] Trial 87 finished with value: 3.833673469387755 and parameters: {'epochs': 197, 'learning_rate': 0.0179764451477874}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:20,348] Trial 88 finished with value: 4.093294460641399 and parameters: {'epochs': 316, 'learning_rate': 0.015800242090774335}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:21,659] Trial 89 finished with value: 4.093294460641399 and parameters: {'epochs': 258, 'learning_rate': 0.020420698241514317}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:23,633] Trial 90 finished with value: 4.093294460641399 and parameters: {'epochs': 388, 'learning_rate': 0.012849970376800178}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:24,214] Trial 91 finished with value: 3.833673469387755 and parameters: {'epochs': 112, 'learning_rate': 0.0588430296756379}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:24,747] Trial 92 finished with value: 4.093294460641399 and parameters: {'epochs': 103, 'learning_rate': 0.04957596890034628}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:25,416] Trial 93 finished with value: 3.833673469387755 and parameters: {'epochs': 130, 'learning_rate': 0.07135477586704977}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:27,717] Trial 94 finished with value: 3.833673469387755 and parameters: {'epochs': 439, 'learning_rate': 0.03224633221036513}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:28,454] Trial 95 finished with value: 3.833673469387755 and parameters: {'epochs': 146, 'learning_rate': 0.02555715666844406}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:29,402] Trial 96 finished with value: 4.093294460641399 and parameters: {'epochs': 187, 'learning_rate': 0.023397232942056017}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:30,547] Trial 97 finished with value: 4.093294460641399 and parameters: {'epochs': 226, 'learning_rate': 0.016833165236238935}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:31,940] Trial 98 finished with value: 4.093294460641399 and parameters: {'epochs': 273, 'learning_rate': 0.01997431916297577}. Best is trial 2 with value: 4.093294460641399.\n",
      "[I 2023-07-29 16:14:36,284] Trial 99 finished with value: 4.093294460641399 and parameters: {'epochs': 856, 'learning_rate': 0.007435206811087596}. Best is trial 2 with value: 4.093294460641399.\n"
     ]
    }
   ],
   "source": [
    "from pu_modified_lr.mlr import ModifiedLogisticRegression\n",
    "\n",
    "\n",
    "def mlr_objective(trial):\n",
    "    classifier_obj = ModifiedLogisticRegression(\n",
    "        epochs=trial.suggest_int(\"epochs\", 100, 1000),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1),\n",
    "    )\n",
    "    classifier_obj.fit(X_train_scaled, y_train)\n",
    "    y_pred = classifier_obj.predict(X_val_scaled)\n",
    "    return lee_liu_score(y_known=y_val, y_pred=y_pred)\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=RANDOM_SEED)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(mlr_objective, n_trials=NUM_TRIALS)\n",
    "best_params = study.best_params\n",
    "optimised_mlr = ModifiedLogisticRegression(**best_params)\n",
    "optimised_models.append(optimised_mlr)\n",
    "models_info.append(\n",
    "    {\"model\": \"MLR\", \"params\": best_params, \"ll_score\": study.best_value}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_to_alpha(y_labels, c):\n",
    "    # As per result in Bekker et al. 2020\n",
    "    prob_labelled = sum(y_labels == 1) / len(y_labels)\n",
    "    alpha = prob_labelled / c   \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-29 16:14:36,297] A new study created in memory with name: no-name-e6a472d9-5b04-41c9-be61-6d17a01ec08c\n",
      "[I 2023-07-29 16:14:38,708] Trial 0 finished with value: 3.7741935483870965 and parameters: {'n_estimators': 90, 'risk_estimator': 'nnPU', 'loss': 'quadratic', 'min_samples_leaf': 2, 'max_features': 'all', 'max_candidates': 7}. Best is trial 0 with value: 3.7741935483870965.\n",
      "[I 2023-07-29 16:14:52,229] Trial 1 finished with value: 3.4383673469387754 and parameters: {'n_estimators': 149, 'risk_estimator': 'uPU', 'loss': 'quadratic', 'min_samples_leaf': 2, 'max_features': 'all', 'max_candidates': 6}. Best is trial 0 with value: 3.7741935483870965.\n",
      "[I 2023-07-29 16:14:54,311] Trial 2 finished with value: 3.9000000000000004 and parameters: {'n_estimators': 101, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 4, 'max_features': 'all', 'max_candidates': 2}. Best is trial 2 with value: 3.9000000000000004.\n",
      "[I 2023-07-29 16:14:58,083] Trial 3 finished with value: 3.9000000000000004 and parameters: {'n_estimators': 115, 'risk_estimator': 'nnPU', 'loss': 'quadratic', 'min_samples_leaf': 1, 'max_features': 'all', 'max_candidates': 9}. Best is trial 2 with value: 3.9000000000000004.\n",
      "[I 2023-07-29 16:15:01,362] Trial 4 finished with value: 3.4383673469387754 and parameters: {'n_estimators': 78, 'risk_estimator': 'uPU', 'loss': 'quadratic', 'min_samples_leaf': 5, 'max_features': 'all', 'max_candidates': 3}. Best is trial 2 with value: 3.9000000000000004.\n",
      "[I 2023-07-29 16:15:13,818] Trial 5 finished with value: 3.4383673469387754 and parameters: {'n_estimators': 141, 'risk_estimator': 'uPU', 'loss': 'quadratic', 'min_samples_leaf': 10, 'max_features': 'all', 'max_candidates': 9}. Best is trial 2 with value: 3.9000000000000004.\n",
      "[I 2023-07-29 16:15:15,631] Trial 6 finished with value: 4.0344827586206895 and parameters: {'n_estimators': 130, 'risk_estimator': 'nnPU', 'loss': 'quadratic', 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_candidates': 9}. Best is trial 6 with value: 4.0344827586206895.\n",
      "[I 2023-07-29 16:15:16,511] Trial 7 finished with value: 4.333333333333333 and parameters: {'n_estimators': 87, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 7 with value: 4.333333333333333.\n",
      "[I 2023-07-29 16:15:16,840] Trial 8 finished with value: 3.7741935483870965 and parameters: {'n_estimators': 25, 'risk_estimator': 'nnPU', 'loss': 'logistic', 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_candidates': 9}. Best is trial 7 with value: 4.333333333333333.\n",
      "[I 2023-07-29 16:15:19,010] Trial 9 finished with value: 3.7741935483870965 and parameters: {'n_estimators': 134, 'risk_estimator': 'nnPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'all', 'max_candidates': 5}. Best is trial 7 with value: 4.333333333333333.\n",
      "[I 2023-07-29 16:15:20,329] Trial 10 finished with value: 4.5 and parameters: {'n_estimators': 191, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:21,807] Trial 11 finished with value: 4.5 and parameters: {'n_estimators': 197, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:23,170] Trial 12 finished with value: 4.5 and parameters: {'n_estimators': 196, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:25,616] Trial 13 finished with value: 4.333333333333333 and parameters: {'n_estimators': 194, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 4}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:26,757] Trial 14 finished with value: 4.244897959183674 and parameters: {'n_estimators': 175, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 9, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:28,527] Trial 15 finished with value: 4.333333333333333 and parameters: {'n_estimators': 167, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 3}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:29,630] Trial 16 finished with value: 4.081632653061225 and parameters: {'n_estimators': 169, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:30,429] Trial 17 finished with value: 3.9304610733182157 and parameters: {'n_estimators': 62, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 4}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:33,650] Trial 18 finished with value: 3.790087463556851 and parameters: {'n_estimators': 182, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 10, 'max_features': 'sqrt', 'max_candidates': 7}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:34,970] Trial 19 finished with value: 4.333333333333333 and parameters: {'n_estimators': 157, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:37,157] Trial 20 finished with value: 4.178571428571429 and parameters: {'n_estimators': 200, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_candidates': 3}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:38,487] Trial 21 finished with value: 4.5 and parameters: {'n_estimators': 188, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:39,884] Trial 22 finished with value: 4.5 and parameters: {'n_estimators': 200, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:41,502] Trial 23 finished with value: 4.5 and parameters: {'n_estimators': 180, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:43,462] Trial 24 finished with value: 4.333333333333333 and parameters: {'n_estimators': 160, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 9, 'max_features': 'sqrt', 'max_candidates': 4}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:44,724] Trial 25 finished with value: 4.5 and parameters: {'n_estimators': 182, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:45,640] Trial 26 finished with value: 4.178571428571429 and parameters: {'n_estimators': 154, 'risk_estimator': 'nnPU', 'loss': 'logistic', 'min_samples_leaf': 9, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:47,810] Trial 27 finished with value: 4.333333333333333 and parameters: {'n_estimators': 188, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_candidates': 3}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:50,294] Trial 28 finished with value: 3.6836734693877555 and parameters: {'n_estimators': 169, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 5}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:52,076] Trial 29 finished with value: 3.9000000000000004 and parameters: {'n_estimators': 129, 'risk_estimator': 'nnPU', 'loss': 'quadratic', 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_candidates': 10}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:52,317] Trial 30 finished with value: 3.9304610733182157 and parameters: {'n_estimators': 28, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:53,584] Trial 31 finished with value: 4.5 and parameters: {'n_estimators': 190, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:55,216] Trial 32 finished with value: 4.333333333333333 and parameters: {'n_estimators': 191, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:15:56,455] Trial 33 finished with value: 4.333333333333333 and parameters: {'n_estimators': 176, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:00,358] Trial 34 finished with value: 3.4383673469387754 and parameters: {'n_estimators': 147, 'risk_estimator': 'uPU', 'loss': 'quadratic', 'min_samples_leaf': 8, 'max_features': 'all', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:06,993] Trial 35 finished with value: 3.4205539358600583 and parameters: {'n_estimators': 118, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'all', 'max_candidates': 7}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:09,702] Trial 36 finished with value: 3.9072356215213353 and parameters: {'n_estimators': 164, 'risk_estimator': 'uPU', 'loss': 'quadratic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 3}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:11,594] Trial 37 finished with value: 3.9000000000000004 and parameters: {'n_estimators': 198, 'risk_estimator': 'nnPU', 'loss': 'logistic', 'min_samples_leaf': 9, 'max_features': 'all', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:14,437] Trial 38 finished with value: 3.9072356215213353 and parameters: {'n_estimators': 102, 'risk_estimator': 'uPU', 'loss': 'quadratic', 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_candidates': 6}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:15,676] Trial 39 finished with value: 4.081632653061225 and parameters: {'n_estimators': 185, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:17,486] Trial 40 finished with value: 4.0344827586206895 and parameters: {'n_estimators': 146, 'risk_estimator': 'nnPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'all', 'max_candidates': 3}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:18,821] Trial 41 finished with value: 4.5 and parameters: {'n_estimators': 196, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:20,074] Trial 42 finished with value: 4.333333333333333 and parameters: {'n_estimators': 174, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:21,839] Trial 43 finished with value: 4.333333333333333 and parameters: {'n_estimators': 200, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:23,074] Trial 44 finished with value: 4.081632653061225 and parameters: {'n_estimators': 189, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:24,735] Trial 45 finished with value: 4.5 and parameters: {'n_estimators': 182, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:26,820] Trial 46 finished with value: 3.4383673469387754 and parameters: {'n_estimators': 68, 'risk_estimator': 'uPU', 'loss': 'quadratic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 8}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:27,922] Trial 47 finished with value: 4.244897959183674 and parameters: {'n_estimators': 172, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 9, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:31,774] Trial 48 finished with value: 4.0344827586206895 and parameters: {'n_estimators': 191, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 4, 'max_features': 'all', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:33,242] Trial 49 finished with value: 4.0344827586206895 and parameters: {'n_estimators': 194, 'risk_estimator': 'nnPU', 'loss': 'logistic', 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_candidates': 3}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:34,412] Trial 50 finished with value: 4.081632653061225 and parameters: {'n_estimators': 176, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:36,054] Trial 51 finished with value: 4.5 and parameters: {'n_estimators': 181, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:37,562] Trial 52 finished with value: 4.5 and parameters: {'n_estimators': 200, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:38,822] Trial 53 finished with value: 4.5 and parameters: {'n_estimators': 182, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:40,327] Trial 54 finished with value: 4.5 and parameters: {'n_estimators': 166, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:41,691] Trial 55 finished with value: 4.5 and parameters: {'n_estimators': 188, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:43,016] Trial 56 finished with value: 4.333333333333333 and parameters: {'n_estimators': 154, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:44,535] Trial 57 finished with value: 4.333333333333333 and parameters: {'n_estimators': 176, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:46,693] Trial 58 finished with value: 4.333333333333333 and parameters: {'n_estimators': 161, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 4}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:47,823] Trial 59 finished with value: 4.0344827586206895 and parameters: {'n_estimators': 195, 'risk_estimator': 'nnPU', 'loss': 'quadratic', 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:48,339] Trial 60 finished with value: 3.6836734693877555 and parameters: {'n_estimators': 46, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 3}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:49,591] Trial 61 finished with value: 4.5 and parameters: {'n_estimators': 181, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:51,056] Trial 62 finished with value: 4.333333333333333 and parameters: {'n_estimators': 186, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:52,426] Trial 63 finished with value: 4.5 and parameters: {'n_estimators': 192, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:53,956] Trial 64 finished with value: 4.333333333333333 and parameters: {'n_estimators': 178, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:55,162] Trial 65 finished with value: 4.5 and parameters: {'n_estimators': 171, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:16:59,265] Trial 66 finished with value: 4.0344827586206895 and parameters: {'n_estimators': 200, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 3, 'max_features': 'all', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:00,295] Trial 67 finished with value: 4.178571428571429 and parameters: {'n_estimators': 136, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:01,976] Trial 68 finished with value: 4.333333333333333 and parameters: {'n_estimators': 186, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:05,875] Trial 69 finished with value: 3.737355811889973 and parameters: {'n_estimators': 193, 'risk_estimator': 'uPU', 'loss': 'quadratic', 'min_samples_leaf': 9, 'max_features': 'sqrt', 'max_candidates': 5}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:06,463] Trial 70 finished with value: 4.178571428571429 and parameters: {'n_estimators': 93, 'risk_estimator': 'nnPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:07,790] Trial 71 finished with value: 4.5 and parameters: {'n_estimators': 189, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:09,052] Trial 72 finished with value: 4.5 and parameters: {'n_estimators': 183, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:10,465] Trial 73 finished with value: 4.5 and parameters: {'n_estimators': 194, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:12,006] Trial 74 finished with value: 4.5 and parameters: {'n_estimators': 169, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:13,526] Trial 75 finished with value: 4.333333333333333 and parameters: {'n_estimators': 179, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:15,733] Trial 76 finished with value: 4.0344827586206895 and parameters: {'n_estimators': 186, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'all', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:17,228] Trial 77 finished with value: 4.5 and parameters: {'n_estimators': 197, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:20,629] Trial 78 finished with value: 3.7741935483870965 and parameters: {'n_estimators': 163, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 8}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:25,068] Trial 79 finished with value: 3.581632653061224 and parameters: {'n_estimators': 118, 'risk_estimator': 'uPU', 'loss': 'quadratic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 10}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:26,173] Trial 80 finished with value: 4.178571428571429 and parameters: {'n_estimators': 190, 'risk_estimator': 'nnPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:27,488] Trial 81 finished with value: 4.5 and parameters: {'n_estimators': 195, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:29,257] Trial 82 finished with value: 4.333333333333333 and parameters: {'n_estimators': 200, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:30,533] Trial 83 finished with value: 4.5 and parameters: {'n_estimators': 185, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:31,814] Trial 84 finished with value: 4.081632653061225 and parameters: {'n_estimators': 194, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:33,431] Trial 85 finished with value: 4.5 and parameters: {'n_estimators': 174, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:34,720] Trial 86 finished with value: 4.5 and parameters: {'n_estimators': 190, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:38,458] Trial 87 finished with value: 3.9000000000000004 and parameters: {'n_estimators': 197, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 5, 'max_features': 'all', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:41,527] Trial 88 finished with value: 3.9304610733182157 and parameters: {'n_estimators': 180, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 6}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:42,792] Trial 89 finished with value: 4.5 and parameters: {'n_estimators': 185, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:43,910] Trial 90 finished with value: 4.244897959183674 and parameters: {'n_estimators': 158, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:46,259] Trial 91 finished with value: 4.081632653061225 and parameters: {'n_estimators': 178, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 4}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:48,278] Trial 92 finished with value: 4.333333333333333 and parameters: {'n_estimators': 191, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 3}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:50,027] Trial 93 finished with value: 4.333333333333333 and parameters: {'n_estimators': 197, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:51,779] Trial 94 finished with value: 4.5 and parameters: {'n_estimators': 184, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:52,973] Trial 95 finished with value: 4.5 and parameters: {'n_estimators': 173, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:54,301] Trial 96 finished with value: 4.5 and parameters: {'n_estimators': 189, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:55,807] Trial 97 finished with value: 4.178571428571429 and parameters: {'n_estimators': 196, 'risk_estimator': 'nnPU', 'loss': 'quadratic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 3}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:57,242] Trial 98 finished with value: 4.333333333333333 and parameters: {'n_estimators': 166, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_candidates': 2}. Best is trial 10 with value: 4.5.\n",
      "[I 2023-07-29 16:17:58,612] Trial 99 finished with value: 4.5 and parameters: {'n_estimators': 181, 'risk_estimator': 'uPU', 'loss': 'logistic', 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_candidates': 1}. Best is trial 10 with value: 4.5.\n"
     ]
    }
   ],
   "source": [
    "from PUExtraTrees.trees import PUExtraTrees\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def puet_objective(trial):\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    classifier_obj = PUExtraTrees(\n",
    "        n_estimators=trial.suggest_int(\"n_estimators\", 25, 200),\n",
    "        risk_estimator=trial.suggest_categorical(\"risk_estimator\", [\"nnPU\", \"uPU\"]),\n",
    "        loss=trial.suggest_categorical(\"loss\", [\"quadratic\", \"logistic\"]),\n",
    "        min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        max_features=trial.suggest_categorical(\"max_features\", [\"sqrt\", \"all\"]),\n",
    "        max_candidates=trial.suggest_int(\"max_candidates\", 1, 10),\n",
    "    )\n",
    "\n",
    "    optimised_elkanoto.fit(X_train_scaled, y_train)\n",
    "    elkan_c = optimised_elkanoto.c\n",
    "    elkan_alpha = c_to_alpha(y, elkan_c)\n",
    "\n",
    "    classifier_obj.fit(X_train_scaled, y_train, alpha=elkan_alpha)\n",
    "    y_pred = classifier_obj.predict(X_val_scaled)\n",
    "    return lee_liu_score(y_known=y_val, y_pred=y_pred)\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=RANDOM_SEED)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(puet_objective, n_trials=NUM_TRIALS)\n",
    "best_params = study.best_params\n",
    "optimised = PUExtraTrees(**best_params)\n",
    "optimised_models.append(optimised)\n",
    "models_info.append(\n",
    "    {\"model\": \"PUExtraTrees\", \"params\": best_params, \"ll_score\": study.best_value}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>ll_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Classic Elkanoto</td>\n",
       "      <td>{'base_clf_option': 'LDA', 'hold_out_ratio': 0...</td>\n",
       "      <td>4.823748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PUExtraTrees</td>\n",
       "      <td>{'n_estimators': 191, 'risk_estimator': 'uPU',...</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>{'base_clf_option': 'SVM', 'num_bagged': 156, ...</td>\n",
       "      <td>4.353432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLR</td>\n",
       "      <td>{'epochs': 240, 'learning_rate': 0.01568385258...</td>\n",
       "      <td>4.093294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weighted Elkanoto</td>\n",
       "      <td>{'base_clf_option': 'Logistic Regression', 'ho...</td>\n",
       "      <td>3.078947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model                                             params  \\\n",
       "0    Classic Elkanoto  {'base_clf_option': 'LDA', 'hold_out_ratio': 0...   \n",
       "4       PUExtraTrees  {'n_estimators': 191, 'risk_estimator': 'uPU',...   \n",
       "2  Bagging Classifier  {'base_clf_option': 'SVM', 'num_bagged': 156, ...   \n",
       "3                 MLR  {'epochs': 240, 'learning_rate': 0.01568385258...   \n",
       "1   Weighted Elkanoto  {'base_clf_option': 'Logistic Regression', 'ho...   \n",
       "\n",
       "   ll_score  \n",
       "0  4.823748  \n",
       "4  4.500000  \n",
       "2  4.353432  \n",
       "3  4.093294  \n",
       "1  3.078947  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df = pd.DataFrame(models_info)\n",
    "models_df.sort_values(\"ll_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ElkanotoPuClassifier\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "\n",
      "\n",
      "Training WeightedElkanotoPuClassifier\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "\n",
      "\n",
      "Training BaggingPuClassifier\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "\n",
      "\n",
      "Training ModifiedLogisticRegression\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "\n",
      "\n",
      "Training PUExtraTrees\n",
      "Trial 0\n",
      "Trial 100\n",
      "Trial 200\n",
      "Trial 300\n",
      "Trial 400\n",
      "Trial 500\n",
      "Trial 600\n",
      "Trial 700\n",
      "Trial 800\n",
      "Trial 900\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rough sanity check performance of models on full dataset\n",
    "import time\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "models_fittimes = [[] for _ in range(len(optimised_models))]\n",
    "models_recalls = [[] for _ in range(len(optimised_models))]\n",
    "models_llscores = [[] for _ in range(len(optimised_models))]\n",
    "models_frac_pos = [[] for _ in range(len(optimised_models))]\n",
    "\n",
    "for i, model in enumerate(optimised_models):\n",
    "    print(f\"Training {model.__class__.__name__}\")\n",
    "    for j in range(1000):\n",
    "        if j % 100 == 0: print(f'Trial {j}')\n",
    "        X_train, X_val, y_train,y_val_ = get_subburst_preserved_train_test(\n",
    "        original_df=df, X=X, y=y, test_size=0.2\n",
    "        )\n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        X_val = X_val.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        y_val = y_val_.reset_index(drop=True)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "        start = time.time()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        end = time.time()\n",
    "        models_fittimes[i].append(end - start)\n",
    "\n",
    "        predictions = model.predict(X_val_scaled)\n",
    "        recall = recall_score(y_val, predictions)\n",
    "        ll_score = lee_liu_score(y_val, predictions)\n",
    "        frac_pos = np.mean(predictions)\n",
    "        \n",
    "        models_recalls[i].append(recall)\n",
    "        models_llscores[i].append(ll_score)\n",
    "        models_frac_pos[i].append(frac_pos)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>ll_score</th>\n",
       "      <th>recall_mean</th>\n",
       "      <th>recall_std</th>\n",
       "      <th>llscore_mean</th>\n",
       "      <th>llscore_std</th>\n",
       "      <th>frac_pos_mean</th>\n",
       "      <th>frac_pos_std</th>\n",
       "      <th>fittime_mean</th>\n",
       "      <th>fittime_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagging Classifier</td>\n",
       "      <td>{'base_clf_option': 'SVM', 'num_bagged': 156, ...</td>\n",
       "      <td>4.353432</td>\n",
       "      <td>0.768886</td>\n",
       "      <td>0.108086</td>\n",
       "      <td>3.827464</td>\n",
       "      <td>0.743272</td>\n",
       "      <td>0.158227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.773127</td>\n",
       "      <td>35.515218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLR</td>\n",
       "      <td>{'epochs': 240, 'learning_rate': 0.01568385258...</td>\n",
       "      <td>4.093294</td>\n",
       "      <td>0.690148</td>\n",
       "      <td>0.131132</td>\n",
       "      <td>3.770481</td>\n",
       "      <td>0.788244</td>\n",
       "      <td>0.129518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.229824</td>\n",
       "      <td>0.039186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Classic Elkanoto</td>\n",
       "      <td>{'base_clf_option': 'LDA', 'hold_out_ratio': 0...</td>\n",
       "      <td>4.823748</td>\n",
       "      <td>0.697638</td>\n",
       "      <td>0.140524</td>\n",
       "      <td>3.667550</td>\n",
       "      <td>0.871950</td>\n",
       "      <td>0.136433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.000158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PUExtraTrees</td>\n",
       "      <td>{'n_estimators': 191, 'risk_estimator': 'uPU',...</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.826233</td>\n",
       "      <td>0.092694</td>\n",
       "      <td>3.585413</td>\n",
       "      <td>0.673606</td>\n",
       "      <td>0.194876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.311885</td>\n",
       "      <td>0.080238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weighted Elkanoto</td>\n",
       "      <td>{'base_clf_option': 'Logistic Regression', 'ho...</td>\n",
       "      <td>3.078947</td>\n",
       "      <td>0.896357</td>\n",
       "      <td>0.123922</td>\n",
       "      <td>3.011100</td>\n",
       "      <td>0.914396</td>\n",
       "      <td>0.316751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model                                             params  \\\n",
       "2  Bagging Classifier  {'base_clf_option': 'SVM', 'num_bagged': 156, ...   \n",
       "3                 MLR  {'epochs': 240, 'learning_rate': 0.01568385258...   \n",
       "0    Classic Elkanoto  {'base_clf_option': 'LDA', 'hold_out_ratio': 0...   \n",
       "4       PUExtraTrees  {'n_estimators': 191, 'risk_estimator': 'uPU',...   \n",
       "1   Weighted Elkanoto  {'base_clf_option': 'Logistic Regression', 'ho...   \n",
       "\n",
       "   ll_score  recall_mean  recall_std  llscore_mean  llscore_std  \\\n",
       "2  4.353432     0.768886    0.108086      3.827464     0.743272   \n",
       "3  4.093294     0.690148    0.131132      3.770481     0.788244   \n",
       "0  4.823748     0.697638    0.140524      3.667550     0.871950   \n",
       "4  4.500000     0.826233    0.092694      3.585413     0.673606   \n",
       "1  3.078947     0.896357    0.123922      3.011100     0.914396   \n",
       "\n",
       "   frac_pos_mean  frac_pos_std  fittime_mean  fittime_std  \n",
       "2       0.158227           0.0      3.773127    35.515218  \n",
       "3       0.129518           0.0      1.229824     0.039186  \n",
       "0       0.136433           0.0      0.000880     0.000158  \n",
       "4       0.194876           0.0      1.311885     0.080238  \n",
       "1       0.316751           0.0      0.001365     0.000216  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_recalls_mean = [np.mean(recalls) for recalls in models_recalls]\n",
    "models_recalls_std = [np.std(recalls) for recalls in models_recalls]\n",
    "models_llscores_mean = [np.mean(llscores) for llscores in models_llscores]\n",
    "models_llscores_std = [np.std(llscores) for llscores in models_llscores]\n",
    "models_frac_pos = [np.mean(frac_pos) for frac_pos in models_frac_pos]\n",
    "models_frac_pos_std = [np.std(frac_pos) for frac_pos in models_frac_pos]\n",
    "models_fittimes_mean = [np.mean(fittimes) for fittimes in models_fittimes]\n",
    "models_fittimes_std = [np.std(fittimes) for fittimes in models_fittimes]\n",
    "\n",
    "models_df[\"recall_mean\"] = models_recalls_mean\n",
    "models_df[\"recall_std\"] = models_recalls_std\n",
    "models_df[\"llscore_mean\"] = models_llscores_mean\n",
    "models_df[\"llscore_std\"] = models_llscores_std\n",
    "models_df[\"frac_pos_mean\"] = models_frac_pos\n",
    "models_df[\"frac_pos_std\"] = models_frac_pos_std\n",
    "models_df[\"fittime_mean\"] = models_fittimes_mean\n",
    "models_df[\"fittime_std\"] = models_fittimes_std\n",
    "\n",
    "\n",
    "models_df.sort_values(\"llscore_mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeater Candidate Identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "predictions = {}\n",
    "\n",
    "import shap\n",
    "\n",
    "\"\"\"\n",
    "1. Train each model 1000 times\n",
    "2. For every trial, run predictions for every FRB in the dataset. If it is predicted as a repeater by 3 or more models, increment its counter by 1\n",
    "3. When the trials are done, filter the list of repeater candidates to those which were identified 100 times\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "MIN_VOTES_FOR_REPEATER_CANDIDATE = 3 # out of 5\n",
    "NUM_ITERATIONS = 1000\n",
    "\n",
    "# For ordering of SHAP figures\n",
    "order = FEATURES\n",
    "cols_nums = {col: i for i, col in enumerate(X.columns)}\n",
    "order = list(map(cols_nums.get, order))\n",
    "\n",
    "explainers = []\n",
    "models_shap_values = []\n",
    "\n",
    "all_priors = []\n",
    "\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    # print(f'Trial {i}')\n",
    "\n",
    "    X_train, _, y_train, _ = get_subburst_preserved_train_test(\n",
    "        original_df=df, X=X, y=y, test_size=0.2\n",
    "    )\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "    model_names = []\n",
    "    models_predictions = []\n",
    "    round_priors = []\n",
    "\n",
    "    for optimised_model in optimised_models:\n",
    "        model_name = optimised_model.__class__.__name__\n",
    "        model_names.append(model_name)\n",
    "\n",
    "        # PUExtraTrees must be after all three of these other classifiers in `optimised_models` for this to work!\n",
    "        if model_name != \"PUExtraTrees\":\n",
    "            optimised_model.fit(X_train_scaled, y_train)\n",
    "            if model_name == \"ElkanotoPuClassifier\":\n",
    "                c = optimised_model.c\n",
    "                alpha = c_to_alpha(y_train, c)\n",
    "                round_priors.append(alpha)\n",
    "            elif model_name == \"WeightedElkanotoPuClassifier\":\n",
    "                c = optimised_model.c\n",
    "                alpha = c_to_alpha(y_train, c)\n",
    "                round_priors.append(alpha)\n",
    "            elif model_name == \"ModifiedLogisticRegression\":\n",
    "                c = optimised_model.c_hat\n",
    "                alpha = c_to_alpha(y_train, c)\n",
    "                round_priors.append(alpha)\n",
    "        else:\n",
    "            mean_alpha = round_priors[1] # we only want alpha from elkanoto\n",
    "            optimised_model.fit(X_train_scaled, y_train, alpha=mean_alpha)\n",
    "        all_priors.append(round_priors)\n",
    "        \n",
    "        # Get predictions to identify potential FRB repeater candidates\n",
    "        preds = optimised_model.predict(X_scaled)\n",
    "        models_predictions.append(preds)\n",
    "\n",
    "        if i == NUM_ITERATIONS - 1:\n",
    "            explainer = shap.Explainer(optimised_model.predict, X_scaled)\n",
    "            explainer.feature_names = FEATURE_LABELS\n",
    "            explainers.append(explainer)\n",
    "            shap_values = explainer(X_scaled)\n",
    "            models_shap_values.append(shap_values)\n",
    "\n",
    "            shap.plots.beeswarm(shap_values, max_display=len(FEATURES), order=order, show=False)\n",
    "            plt.title(f'SHAP beeswarm plot for {model_name}')\n",
    "            plt.savefig(f'./figures/puml/beeswarm_{optimised_model.__class__.__name__}.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "            plt.clf()\n",
    "            shap.plots.heatmap(shap_values, max_display=len(FEATURES), feature_order=order, instance_order=shap_values.sum(1), show=False)\n",
    "            plt.title(f'SHAP heatmap for {model_name}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'./figures/puml/heatmap_{model_name}.png', dpi=300, bbox_inches='tight')\n",
    "            plt.clf()\n",
    "\n",
    "    # For every FRB in the dataset, check if it is predicted as a repeater by 4 or more models\n",
    "    for index, row in X_scaled_df.iterrows():\n",
    "        if df.iloc[index]['is_repeater'] == 1:\n",
    "            continue\n",
    "        \n",
    "        model_votes = 0\n",
    "        for model_preds in models_predictions:\n",
    "            if model_preds[index] == 1:\n",
    "                model_votes += 1\n",
    "\n",
    "        if model_votes >= MIN_VOTES_FOR_REPEATER_CANDIDATE:\n",
    "            tns_name = df['tns_name'][index]\n",
    "            sub_num = df['sub_num'][index]\n",
    "            # print(tns_name)\n",
    "            key = f'{tns_name}_{sub_num}'\n",
    "            if key not in predictions.keys():\n",
    "                predictions[key] = 1\n",
    "            else:\n",
    "                predictions[key] += 1\n",
    "            \n",
    "            if i == NUM_ITERATIONS - 1:\n",
    "                try:\n",
    "                    path = f'./figures/puml/candidate_analysis/{tns_name}_{sub_num}'\n",
    "                    if not os.path.exists(path):\n",
    "                        os.makedirs(f'./figures/puml/candidate_analysis/{tns_name}_{sub_num}')\n",
    "                    for model_num, model_shap_values in enumerate(models_shap_values):\n",
    "                        shap.plots.waterfall(model_shap_values[index], show=False)\n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(f'./figures/puml/candidate_analysis/{tns_name}_{sub_num}/{model_names[model_num]}.png')\n",
    "                        plt.clf()\n",
    "                except Exception as e:\n",
    "                    print(f'ERROR: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_priors = all_priors[:-2]\n",
    "for i, prior in enumerate(all_priors):\n",
    "    if len(prior) != 3:\n",
    "        print(i, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2319819579486149\n",
      "0.22135291988492914\n",
      "[0.16272215]\n"
     ]
    }
   ],
   "source": [
    "elkan_priors = [priors[0] for priors in all_priors]\n",
    "elkan_avg_priors = np.mean(elkan_priors, axis=0)\n",
    "welkan_priors = [priors[1] for priors in all_priors]\n",
    "welkan_avg_priors = np.mean(welkan_priors, axis=0)\n",
    "mlr_priors = [priors[2] for priors in all_priors]\n",
    "mlr_avg_priors = np.mean(mlr_priors, axis=0)\n",
    "print(f'{elkan_avg_priors}\\n{welkan_avg_priors}\\n{mlr_avg_priors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of repeater candidates with >0% confidence: 65\n",
      "Number of repeater candidates with >=10% confidence: 31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tns_name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FRB20190423B_0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FRB20190429B_0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRB20181017B_0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FRB20190422A_1</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FRB20190329A_0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FRB20190423B_1</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FRB20181231B_0</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FRB20181221A_0</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FRB20190112A_0</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FRB20190422A_0</td>\n",
       "      <td>982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FRB20190228A_0</td>\n",
       "      <td>978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FRB20190527A_0</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FRB20190218B_0</td>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FRB20190206A_0</td>\n",
       "      <td>795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FRB20181229B_0</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FRB20190128C_0</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FRB20190409B_0</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FRB20190412B_0</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>FRB20190410A_0</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FRB20190609A_1</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>FRB20190129A_0</td>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FRB20181129B_0</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FRB20190125A_0</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FRB20181228B_0</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FRB20190609A_0</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FRB20180801A_0</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>FRB20181214A_0</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>FRB20190403E_0</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>FRB20190527A_1</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FRB20190601C_0</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>FRB20190601C_1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tns_name  count\n",
       "15  FRB20190423B_0   1000\n",
       "17  FRB20190429B_0   1000\n",
       "1   FRB20181017B_0   1000\n",
       "14  FRB20190422A_1   1000\n",
       "10  FRB20190329A_0   1000\n",
       "16  FRB20190423B_1   1000\n",
       "4   FRB20181231B_0    999\n",
       "3   FRB20181221A_0    998\n",
       "5   FRB20190112A_0    989\n",
       "13  FRB20190422A_0    982\n",
       "9   FRB20190228A_0    978\n",
       "18  FRB20190527A_0    903\n",
       "8   FRB20190218B_0    873\n",
       "7   FRB20190206A_0    795\n",
       "22  FRB20181229B_0    780\n",
       "6   FRB20190128C_0    662\n",
       "11  FRB20190409B_0    637\n",
       "12  FRB20190412B_0    603\n",
       "26  FRB20190410A_0    562\n",
       "20  FRB20190609A_1    562\n",
       "24  FRB20190129A_0    533\n",
       "2   FRB20181129B_0    441\n",
       "23  FRB20190125A_0    373\n",
       "21  FRB20181228B_0    323\n",
       "19  FRB20190609A_0    298\n",
       "0   FRB20180801A_0    287\n",
       "30  FRB20181214A_0    223\n",
       "25  FRB20190403E_0    169\n",
       "27  FRB20190527A_1    163\n",
       "28  FRB20190601C_0    123\n",
       "29  FRB20190601C_1    100"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame.from_dict(predictions, columns=[\"count\"], orient=\"index\")\n",
    "print(f\"Number of repeater candidates with >0% confidence: {len(predictions_df)}\")\n",
    "predictions_df = predictions_df[predictions_df[\"count\"] >= (NUM_ITERATIONS * 0.1)]\n",
    "print(f\"Number of repeater candidates with >=10% confidence: {len(predictions_df)}\")\n",
    "predictions_df[\"tns_name\"] = predictions_df.index\n",
    "predictions_df = predictions_df[[\"tns_name\", \"count\"]]\n",
    "predictions_df.reset_index(drop=True, inplace=True)\n",
    "predictions_df.sort_values(by=\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model detects 26 candidates. Of these,\n",
      "23 appear as repeater candidates in reference papers, out of a total of 64 from both papers\n",
      "19 appear as repeater candidates in the supervised paper, out of a total of 27 from the supervised paper\n",
      "Candidates from original paper not identified by us: missed_candidates=['FRB20181218C', 'FRB20190109B', 'FRB20190531C', 'FRB20181226E', 'FRB20190223A', 'FRB20190220A', 'FRB20190308C', 'FRB20180915B', 'FRB20190430A', 'FRB20190125B', 'FRB20190617B', 'FRB20190625A', 'FRB20190110C', 'FRB20190601B', 'FRB20180928A', 'FRB20190111A', 'FRB20190206B', 'FRB20180911A', 'FRB20181013E', 'FRB20190222B', 'FRB20181125A', 'FRB20181130A', 'FRB20190423A', 'FRB20190308B', 'FRB20190221A', 'FRB20190418A', 'FRB20190323D', 'FRB20180920B', 'FRB20181030E', 'FRB20190618A', 'FRB20180923A', 'FRB20180907E', 'FRB20180923C', 'FRB20190517C', 'FRB20190419A', 'FRB20181220A', 'FRB20181128C', 'FRB20190106B', 'FRB20190617A', 'FRB20190204A', 'FRB20190106A']\n",
      "New candidates identified by us: new_candidate_tns_names=['FRB20181228B', 'FRB20181129B', 'FRB20180801A']\n"
     ]
    }
   ],
   "source": [
    "from utils import SUPERVISED_PAPER_REPEATERS, REFERENCE_PAPER_REPEATERS\n",
    "\n",
    "predicted_frb_candidates = list(\n",
    "    set([tns_name[:-2] for tns_name in list(predictions_df[\"tns_name\"].values)])\n",
    ")\n",
    "all_overlap = set(REFERENCE_PAPER_REPEATERS).intersection(predicted_frb_candidates)\n",
    "supervised_overlap = set(SUPERVISED_PAPER_REPEATERS).intersection(\n",
    "    predicted_frb_candidates\n",
    ")\n",
    "\n",
    "print(f\"Our model detects {len(predicted_frb_candidates)} candidates. Of these,\")\n",
    "print(\n",
    "    f\"{len(all_overlap)} appear as repeater candidates in reference papers, out of a total of {len(REFERENCE_PAPER_REPEATERS)} from both papers\"\n",
    ")\n",
    "print(\n",
    "    f\"{len(supervised_overlap)} appear as repeater candidates in the supervised paper, out of a total of {len(SUPERVISED_PAPER_REPEATERS)} from the supervised paper\"\n",
    ")\n",
    "missed_candidates = list(\n",
    "    set(REFERENCE_PAPER_REPEATERS).difference(predicted_frb_candidates)\n",
    ")\n",
    "print(f\"Candidates from original paper not identified by us: {missed_candidates=}\")\n",
    "\n",
    "new_candidate_tns_names = list(\n",
    "    set(predicted_frb_candidates).difference(REFERENCE_PAPER_REPEATERS)\n",
    ")\n",
    "print(f\"New candidates identified by us: {new_candidate_tns_names=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tns_name</th>\n",
       "      <th>sub_num</th>\n",
       "      <th>count</th>\n",
       "      <th>is_new_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FRB20180801A</td>\n",
       "      <td>0</td>\n",
       "      <td>287</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FRB20181228B</td>\n",
       "      <td>0</td>\n",
       "      <td>323</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FRB20181129B</td>\n",
       "      <td>0</td>\n",
       "      <td>441</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FRB20190423B</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FRB20190409B</td>\n",
       "      <td>0</td>\n",
       "      <td>637</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FRB20190601C</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>FRB20190527A</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>FRB20190403E</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>FRB20181214A</td>\n",
       "      <td>0</td>\n",
       "      <td>223</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FRB20190609A</td>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FRB20190125A</td>\n",
       "      <td>0</td>\n",
       "      <td>373</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>FRB20190129A</td>\n",
       "      <td>0</td>\n",
       "      <td>533</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FRB20190609A</td>\n",
       "      <td>1</td>\n",
       "      <td>562</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>FRB20190410A</td>\n",
       "      <td>0</td>\n",
       "      <td>562</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FRB20190412B</td>\n",
       "      <td>0</td>\n",
       "      <td>603</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FRB20190128C</td>\n",
       "      <td>0</td>\n",
       "      <td>662</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FRB20190429B</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FRB20181229B</td>\n",
       "      <td>0</td>\n",
       "      <td>780</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FRB20190206A</td>\n",
       "      <td>0</td>\n",
       "      <td>795</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FRB20190218B</td>\n",
       "      <td>0</td>\n",
       "      <td>873</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FRB20190527A</td>\n",
       "      <td>0</td>\n",
       "      <td>903</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FRB20190228A</td>\n",
       "      <td>0</td>\n",
       "      <td>978</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FRB20190422A</td>\n",
       "      <td>0</td>\n",
       "      <td>982</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FRB20190112A</td>\n",
       "      <td>0</td>\n",
       "      <td>989</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FRB20181221A</td>\n",
       "      <td>0</td>\n",
       "      <td>998</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FRB20181231B</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FRB20190423B</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FRB20190329A</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FRB20190422A</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRB20181017B</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>FRB20190601C</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tns_name  sub_num  count  is_new_candidate\n",
       "0   FRB20180801A        0    287              True\n",
       "21  FRB20181228B        0    323              True\n",
       "2   FRB20181129B        0    441              True\n",
       "15  FRB20190423B        0   1000             False\n",
       "11  FRB20190409B        0    637             False\n",
       "28  FRB20190601C        0    123             False\n",
       "27  FRB20190527A        1    163             False\n",
       "25  FRB20190403E        0    169             False\n",
       "30  FRB20181214A        0    223             False\n",
       "19  FRB20190609A        0    298             False\n",
       "23  FRB20190125A        0    373             False\n",
       "24  FRB20190129A        0    533             False\n",
       "20  FRB20190609A        1    562             False\n",
       "26  FRB20190410A        0    562             False\n",
       "12  FRB20190412B        0    603             False\n",
       "6   FRB20190128C        0    662             False\n",
       "17  FRB20190429B        0   1000             False\n",
       "22  FRB20181229B        0    780             False\n",
       "7   FRB20190206A        0    795             False\n",
       "8   FRB20190218B        0    873             False\n",
       "18  FRB20190527A        0    903             False\n",
       "9   FRB20190228A        0    978             False\n",
       "13  FRB20190422A        0    982             False\n",
       "5   FRB20190112A        0    989             False\n",
       "3   FRB20181221A        0    998             False\n",
       "4   FRB20181231B        0    999             False\n",
       "16  FRB20190423B        1   1000             False\n",
       "10  FRB20190329A        0   1000             False\n",
       "14  FRB20190422A        1   1000             False\n",
       "1   FRB20181017B        0   1000             False\n",
       "29  FRB20190601C        1    100             False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df[\"is_new_candidate\"] = predictions_df[\"tns_name\"].apply(\n",
    "    lambda x: True if x[:-2] in new_candidate_tns_names else False\n",
    ")\n",
    "predictions_df[\"sub_num\"] = predictions_df[\"tns_name\"].apply(lambda x: int(x[-1]))\n",
    "predictions_df[\"tns_name\"] = predictions_df[\"tns_name\"].apply(lambda x: x[:-2])\n",
    "predictions_df = predictions_df[[\"tns_name\", \"sub_num\", \"count\", \"is_new_candidate\"]]\n",
    "predictions_df.sort_values(by=\"count\", ascending=False).sort_values(\n",
    "    \"is_new_candidate\", ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_df = predictions_df.copy()\n",
    "for index, row in predictions_df.iterrows():\n",
    "    tns_name = row[\"tns_name\"]\n",
    "    sub_num = row[\"sub_num\"]\n",
    "    is_new_candidate = row[\"is_new_candidate\"]\n",
    "    \n",
    "    # Visually mark new candidates wiht a *\n",
    "    if is_new_candidate:\n",
    "        display_df.loc[index, \"tns_name\"] = f'{tns_name}*'\n",
    "\n",
    "display_df = display_df[['tns_name', 'sub_num', 'count']]\n",
    "display_df.sort_values(by=['count'], ascending=False)\n",
    "display_df.to_latex('tables/3__find_candidates_puml_results_all.tex', index=False)\n",
    "\n",
    "# export the rows with is_new_candidate == True\n",
    "display_df['is_new_candidate'] = display_df['tns_name'].str.contains('\\*')\n",
    "display_df = display_df[display_df['is_new_candidate'] == True]\n",
    "display_df = display_df.drop(columns=['is_new_candidate'])\n",
    "display_df.to_latex('tables/3__find_candidates_puml_results_new.tex', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tns_name</th>\n",
       "      <th>sub_num</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>in_duration</th>\n",
       "      <th>log_dm_exc_ne2001</th>\n",
       "      <th>scat_time</th>\n",
       "      <th>sp_run</th>\n",
       "      <th>log_peak_freq</th>\n",
       "      <th>log_fre_width</th>\n",
       "      <th>log_T_B</th>\n",
       "      <th>log_energy</th>\n",
       "      <th>log_luminosity</th>\n",
       "      <th>count</th>\n",
       "      <th>is_new_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FRB20180801A</td>\n",
       "      <td>0</td>\n",
       "      <td>322.53</td>\n",
       "      <td>72.72</td>\n",
       "      <td>0.373432</td>\n",
       "      <td>2.752509</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>-75.5</td>\n",
       "      <td>2.774955</td>\n",
       "      <td>2.511570</td>\n",
       "      <td>34.397642</td>\n",
       "      <td>40.597390</td>\n",
       "      <td>42.936302</td>\n",
       "      <td>287</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRB20181017B</td>\n",
       "      <td>0</td>\n",
       "      <td>237.76</td>\n",
       "      <td>78.50</td>\n",
       "      <td>1.914356</td>\n",
       "      <td>2.421110</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>-77.0</td>\n",
       "      <td>2.773201</td>\n",
       "      <td>2.394401</td>\n",
       "      <td>33.269974</td>\n",
       "      <td>39.627719</td>\n",
       "      <td>41.921701</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FRB20181129B</td>\n",
       "      <td>0</td>\n",
       "      <td>307.56</td>\n",
       "      <td>81.32</td>\n",
       "      <td>0.279727</td>\n",
       "      <td>2.536306</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>-112.0</td>\n",
       "      <td>2.745699</td>\n",
       "      <td>2.319572</td>\n",
       "      <td>35.864260</td>\n",
       "      <td>40.103427</td>\n",
       "      <td>42.842131</td>\n",
       "      <td>441</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FRB20181221A</td>\n",
       "      <td>0</td>\n",
       "      <td>230.58</td>\n",
       "      <td>25.86</td>\n",
       "      <td>0.607968</td>\n",
       "      <td>2.465085</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>-128.0</td>\n",
       "      <td>2.707655</td>\n",
       "      <td>2.230845</td>\n",
       "      <td>34.436639</td>\n",
       "      <td>39.647526</td>\n",
       "      <td>42.074499</td>\n",
       "      <td>998</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FRB20181231B</td>\n",
       "      <td>0</td>\n",
       "      <td>128.77</td>\n",
       "      <td>55.99</td>\n",
       "      <td>0.316091</td>\n",
       "      <td>2.176959</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>2.818028</td>\n",
       "      <td>2.441788</td>\n",
       "      <td>33.365483</td>\n",
       "      <td>38.216509</td>\n",
       "      <td>40.824502</td>\n",
       "      <td>999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FRB20190112A</td>\n",
       "      <td>0</td>\n",
       "      <td>257.98</td>\n",
       "      <td>61.20</td>\n",
       "      <td>1.217017</td>\n",
       "      <td>2.584105</td>\n",
       "      <td>0.011010</td>\n",
       "      <td>-51.4</td>\n",
       "      <td>2.843669</td>\n",
       "      <td>2.501723</td>\n",
       "      <td>33.944709</td>\n",
       "      <td>40.561682</td>\n",
       "      <td>42.627843</td>\n",
       "      <td>989</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FRB20190128C</td>\n",
       "      <td>0</td>\n",
       "      <td>69.80</td>\n",
       "      <td>78.94</td>\n",
       "      <td>5.232715</td>\n",
       "      <td>2.378943</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>2.691612</td>\n",
       "      <td>2.377493</td>\n",
       "      <td>32.941016</td>\n",
       "      <td>39.366376</td>\n",
       "      <td>41.517636</td>\n",
       "      <td>662</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FRB20190206A</td>\n",
       "      <td>0</td>\n",
       "      <td>244.85</td>\n",
       "      <td>9.36</td>\n",
       "      <td>0.757227</td>\n",
       "      <td>2.167022</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>-65.7</td>\n",
       "      <td>2.727948</td>\n",
       "      <td>2.330090</td>\n",
       "      <td>33.079954</td>\n",
       "      <td>38.655887</td>\n",
       "      <td>40.869004</td>\n",
       "      <td>795</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FRB20190218B</td>\n",
       "      <td>0</td>\n",
       "      <td>268.70</td>\n",
       "      <td>17.93</td>\n",
       "      <td>1.422016</td>\n",
       "      <td>2.668665</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>2.769377</td>\n",
       "      <td>2.523963</td>\n",
       "      <td>33.407750</td>\n",
       "      <td>40.263788</td>\n",
       "      <td>42.407660</td>\n",
       "      <td>873</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FRB20190228A</td>\n",
       "      <td>0</td>\n",
       "      <td>183.48</td>\n",
       "      <td>22.90</td>\n",
       "      <td>1.648471</td>\n",
       "      <td>2.600864</td>\n",
       "      <td>0.018910</td>\n",
       "      <td>-51.9</td>\n",
       "      <td>2.822626</td>\n",
       "      <td>2.553071</td>\n",
       "      <td>33.154650</td>\n",
       "      <td>40.928775</td>\n",
       "      <td>42.762846</td>\n",
       "      <td>978</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FRB20190329A</td>\n",
       "      <td>0</td>\n",
       "      <td>65.54</td>\n",
       "      <td>73.63</td>\n",
       "      <td>1.037667</td>\n",
       "      <td>2.003461</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>-272.0</td>\n",
       "      <td>2.635785</td>\n",
       "      <td>1.868443</td>\n",
       "      <td>29.341620</td>\n",
       "      <td>35.064458</td>\n",
       "      <td>37.431188</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FRB20190409B</td>\n",
       "      <td>0</td>\n",
       "      <td>126.65</td>\n",
       "      <td>63.47</td>\n",
       "      <td>1.990835</td>\n",
       "      <td>2.376212</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>-34.1</td>\n",
       "      <td>2.736795</td>\n",
       "      <td>2.527608</td>\n",
       "      <td>32.006923</td>\n",
       "      <td>39.463969</td>\n",
       "      <td>41.292706</td>\n",
       "      <td>637</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FRB20190412B</td>\n",
       "      <td>0</td>\n",
       "      <td>285.65</td>\n",
       "      <td>19.25</td>\n",
       "      <td>6.702164</td>\n",
       "      <td>2.044932</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>2.602277</td>\n",
       "      <td>2.359055</td>\n",
       "      <td>30.044506</td>\n",
       "      <td>37.415575</td>\n",
       "      <td>39.147168</td>\n",
       "      <td>603</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FRB20190422A</td>\n",
       "      <td>0</td>\n",
       "      <td>48.56</td>\n",
       "      <td>35.15</td>\n",
       "      <td>2.412200</td>\n",
       "      <td>2.571476</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>-46.9</td>\n",
       "      <td>2.796644</td>\n",
       "      <td>2.572135</td>\n",
       "      <td>32.683137</td>\n",
       "      <td>40.230781</td>\n",
       "      <td>42.175333</td>\n",
       "      <td>982</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FRB20190422A</td>\n",
       "      <td>1</td>\n",
       "      <td>48.56</td>\n",
       "      <td>35.15</td>\n",
       "      <td>1.730491</td>\n",
       "      <td>2.571476</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>-63.7</td>\n",
       "      <td>2.786964</td>\n",
       "      <td>2.495029</td>\n",
       "      <td>32.702496</td>\n",
       "      <td>40.221102</td>\n",
       "      <td>42.165654</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FRB20190423B</td>\n",
       "      <td>0</td>\n",
       "      <td>298.58</td>\n",
       "      <td>26.19</td>\n",
       "      <td>2.482321</td>\n",
       "      <td>2.009876</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>-106.0</td>\n",
       "      <td>2.730459</td>\n",
       "      <td>2.203557</td>\n",
       "      <td>29.811891</td>\n",
       "      <td>35.931427</td>\n",
       "      <td>38.027190</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FRB20190423B</td>\n",
       "      <td>1</td>\n",
       "      <td>298.58</td>\n",
       "      <td>26.19</td>\n",
       "      <td>8.473785</td>\n",
       "      <td>2.009876</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>-116.0</td>\n",
       "      <td>2.719828</td>\n",
       "      <td>2.173068</td>\n",
       "      <td>29.833152</td>\n",
       "      <td>35.920796</td>\n",
       "      <td>38.016559</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FRB20190429B</td>\n",
       "      <td>0</td>\n",
       "      <td>329.93</td>\n",
       "      <td>3.96</td>\n",
       "      <td>5.341601</td>\n",
       "      <td>2.403978</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>-910.0</td>\n",
       "      <td>2.625724</td>\n",
       "      <td>1.704515</td>\n",
       "      <td>33.121116</td>\n",
       "      <td>39.311452</td>\n",
       "      <td>41.558863</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FRB20190527A</td>\n",
       "      <td>0</td>\n",
       "      <td>12.45</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1.737460</td>\n",
       "      <td>2.741073</td>\n",
       "      <td>0.005080</td>\n",
       "      <td>-122.0</td>\n",
       "      <td>2.685473</td>\n",
       "      <td>2.312728</td>\n",
       "      <td>32.649568</td>\n",
       "      <td>40.587738</td>\n",
       "      <td>42.442111</td>\n",
       "      <td>903</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FRB20190609A</td>\n",
       "      <td>0</td>\n",
       "      <td>345.30</td>\n",
       "      <td>87.94</td>\n",
       "      <td>0.359982</td>\n",
       "      <td>2.411956</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>-84.0</td>\n",
       "      <td>2.762904</td>\n",
       "      <td>2.363409</td>\n",
       "      <td>34.621568</td>\n",
       "      <td>39.792412</td>\n",
       "      <td>42.410885</td>\n",
       "      <td>298</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FRB20190609A</td>\n",
       "      <td>1</td>\n",
       "      <td>345.30</td>\n",
       "      <td>87.94</td>\n",
       "      <td>1.766576</td>\n",
       "      <td>2.411956</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>2.778513</td>\n",
       "      <td>2.428092</td>\n",
       "      <td>34.590349</td>\n",
       "      <td>39.808022</td>\n",
       "      <td>42.426495</td>\n",
       "      <td>562</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FRB20181228B</td>\n",
       "      <td>0</td>\n",
       "      <td>250.43</td>\n",
       "      <td>63.85</td>\n",
       "      <td>0.066144</td>\n",
       "      <td>2.723209</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>-353.0</td>\n",
       "      <td>2.638689</td>\n",
       "      <td>2.026468</td>\n",
       "      <td>34.758913</td>\n",
       "      <td>39.717039</td>\n",
       "      <td>42.275895</td>\n",
       "      <td>323</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FRB20181229B</td>\n",
       "      <td>0</td>\n",
       "      <td>238.37</td>\n",
       "      <td>19.78</td>\n",
       "      <td>2.545968</td>\n",
       "      <td>2.555940</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>2.648848</td>\n",
       "      <td>2.189784</td>\n",
       "      <td>33.092143</td>\n",
       "      <td>39.772525</td>\n",
       "      <td>41.826064</td>\n",
       "      <td>780</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FRB20190125A</td>\n",
       "      <td>0</td>\n",
       "      <td>45.73</td>\n",
       "      <td>27.81</td>\n",
       "      <td>2.162394</td>\n",
       "      <td>2.702689</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>-37.0</td>\n",
       "      <td>2.816573</td>\n",
       "      <td>2.634118</td>\n",
       "      <td>33.426823</td>\n",
       "      <td>40.038023</td>\n",
       "      <td>42.362821</td>\n",
       "      <td>373</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>FRB20190129A</td>\n",
       "      <td>0</td>\n",
       "      <td>45.06</td>\n",
       "      <td>21.42</td>\n",
       "      <td>0.805129</td>\n",
       "      <td>2.636187</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>-37.8</td>\n",
       "      <td>2.849849</td>\n",
       "      <td>2.540613</td>\n",
       "      <td>33.701728</td>\n",
       "      <td>40.191410</td>\n",
       "      <td>42.329849</td>\n",
       "      <td>533</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>FRB20190403E</td>\n",
       "      <td>0</td>\n",
       "      <td>220.22</td>\n",
       "      <td>86.54</td>\n",
       "      <td>2.001322</td>\n",
       "      <td>2.246252</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>-36.2</td>\n",
       "      <td>2.792812</td>\n",
       "      <td>2.541617</td>\n",
       "      <td>32.813255</td>\n",
       "      <td>40.061666</td>\n",
       "      <td>41.813023</td>\n",
       "      <td>169</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>FRB20190410A</td>\n",
       "      <td>0</td>\n",
       "      <td>263.47</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>0.941438</td>\n",
       "      <td>2.191730</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>-85.0</td>\n",
       "      <td>2.712397</td>\n",
       "      <td>2.262254</td>\n",
       "      <td>33.178027</td>\n",
       "      <td>38.589899</td>\n",
       "      <td>41.058398</td>\n",
       "      <td>562</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>FRB20190527A</td>\n",
       "      <td>1</td>\n",
       "      <td>12.45</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1.607313</td>\n",
       "      <td>2.741073</td>\n",
       "      <td>0.005080</td>\n",
       "      <td>-133.0</td>\n",
       "      <td>2.652343</td>\n",
       "      <td>2.235814</td>\n",
       "      <td>32.715828</td>\n",
       "      <td>40.554608</td>\n",
       "      <td>42.408981</td>\n",
       "      <td>163</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FRB20190601C</td>\n",
       "      <td>0</td>\n",
       "      <td>88.52</td>\n",
       "      <td>28.47</td>\n",
       "      <td>0.581997</td>\n",
       "      <td>2.376029</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>-68.8</td>\n",
       "      <td>2.713491</td>\n",
       "      <td>2.349346</td>\n",
       "      <td>34.008461</td>\n",
       "      <td>39.370964</td>\n",
       "      <td>41.798246</td>\n",
       "      <td>123</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>FRB20190601C</td>\n",
       "      <td>1</td>\n",
       "      <td>88.52</td>\n",
       "      <td>28.47</td>\n",
       "      <td>0.433945</td>\n",
       "      <td>2.376029</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>-79.5</td>\n",
       "      <td>2.700877</td>\n",
       "      <td>2.305159</td>\n",
       "      <td>34.033689</td>\n",
       "      <td>39.358350</td>\n",
       "      <td>41.785632</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>FRB20181214A</td>\n",
       "      <td>0</td>\n",
       "      <td>70.00</td>\n",
       "      <td>43.07</td>\n",
       "      <td>0.433047</td>\n",
       "      <td>2.453165</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>-139.0</td>\n",
       "      <td>2.638489</td>\n",
       "      <td>2.065164</td>\n",
       "      <td>34.079702</td>\n",
       "      <td>38.391946</td>\n",
       "      <td>41.062479</td>\n",
       "      <td>223</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tns_name  sub_num      ra    dec  in_duration  log_dm_exc_ne2001  \\\n",
       "0   FRB20180801A        0  322.53  72.72     0.373432           2.752509   \n",
       "1   FRB20181017B        0  237.76  78.50     1.914356           2.421110   \n",
       "2   FRB20181129B        0  307.56  81.32     0.279727           2.536306   \n",
       "3   FRB20181221A        0  230.58  25.86     0.607968           2.465085   \n",
       "4   FRB20181231B        0  128.77  55.99     0.316091           2.176959   \n",
       "5   FRB20190112A        0  257.98  61.20     1.217017           2.584105   \n",
       "6   FRB20190128C        0   69.80  78.94     5.232715           2.378943   \n",
       "7   FRB20190206A        0  244.85   9.36     0.757227           2.167022   \n",
       "8   FRB20190218B        0  268.70  17.93     1.422016           2.668665   \n",
       "9   FRB20190228A        0  183.48  22.90     1.648471           2.600864   \n",
       "10  FRB20190329A        0   65.54  73.63     1.037667           2.003461   \n",
       "11  FRB20190409B        0  126.65  63.47     1.990835           2.376212   \n",
       "12  FRB20190412B        0  285.65  19.25     6.702164           2.044932   \n",
       "13  FRB20190422A        0   48.56  35.15     2.412200           2.571476   \n",
       "14  FRB20190422A        1   48.56  35.15     1.730491           2.571476   \n",
       "15  FRB20190423B        0  298.58  26.19     2.482321           2.009876   \n",
       "16  FRB20190423B        1  298.58  26.19     8.473785           2.009876   \n",
       "17  FRB20190429B        0  329.93   3.96     5.341601           2.403978   \n",
       "18  FRB20190527A        0   12.45   7.99     1.737460           2.741073   \n",
       "19  FRB20190609A        0  345.30  87.94     0.359982           2.411956   \n",
       "20  FRB20190609A        1  345.30  87.94     1.766576           2.411956   \n",
       "21  FRB20181228B        0  250.43  63.85     0.066144           2.723209   \n",
       "22  FRB20181229B        0  238.37  19.78     2.545968           2.555940   \n",
       "23  FRB20190125A        0   45.73  27.81     2.162394           2.702689   \n",
       "24  FRB20190129A        0   45.06  21.42     0.805129           2.636187   \n",
       "25  FRB20190403E        0  220.22  86.54     2.001322           2.246252   \n",
       "26  FRB20190410A        0  263.47  -2.37     0.941438           2.191730   \n",
       "27  FRB20190527A        1   12.45   7.99     1.607313           2.741073   \n",
       "28  FRB20190601C        0   88.52  28.47     0.581997           2.376029   \n",
       "29  FRB20190601C        1   88.52  28.47     0.433945           2.376029   \n",
       "30  FRB20181214A        0   70.00  43.07     0.433047           2.453165   \n",
       "\n",
       "    scat_time  sp_run  log_peak_freq  log_fre_width    log_T_B  log_energy  \\\n",
       "0    0.005540   -75.5       2.774955       2.511570  34.397642   40.597390   \n",
       "1    0.004300   -77.0       2.773201       2.394401  33.269974   39.627719   \n",
       "2    0.000830  -112.0       2.745699       2.319572  35.864260   40.103427   \n",
       "3    0.001323  -128.0       2.707655       2.230845  34.436639   39.647526   \n",
       "4    0.001750   -60.0       2.818028       2.441788  33.365483   38.216509   \n",
       "5    0.011010   -51.4       2.843669       2.501723  33.944709   40.561682   \n",
       "6    0.007600   -55.0       2.691612       2.377493  32.941016   39.366376   \n",
       "7    0.002740   -65.7       2.727948       2.330090  33.079954   38.655887   \n",
       "8    0.014100   -60.0       2.769377       2.523963  33.407750   40.263788   \n",
       "9    0.018910   -51.9       2.822626       2.553071  33.154650   40.928775   \n",
       "10   0.000900  -272.0       2.635785       1.868443  29.341620   35.064458   \n",
       "11   0.020900   -34.1       2.736795       2.527608  32.006923   39.463969   \n",
       "12   0.015500    -2.7       2.602277       2.359055  30.044506   37.415575   \n",
       "13   0.002700   -46.9       2.796644       2.572135  32.683137   40.230781   \n",
       "14   0.002700   -63.7       2.786964       2.495029  32.702496   40.221102   \n",
       "15   0.003000  -106.0       2.730459       2.203557  29.811891   35.931427   \n",
       "16   0.003000  -116.0       2.719828       2.173068  29.833152   35.920796   \n",
       "17   0.007800  -910.0       2.625724       1.704515  33.121116   39.311452   \n",
       "18   0.005080  -122.0       2.685473       2.312728  32.649568   40.587738   \n",
       "19   0.000500   -84.0       2.762904       2.363409  34.621568   39.792412   \n",
       "20   0.000500   -67.0       2.778513       2.428092  34.590349   39.808022   \n",
       "21   0.001159  -353.0       2.638689       2.026468  34.758913   39.717039   \n",
       "22   0.005100  -103.0       2.648848       2.189784  33.092143   39.772525   \n",
       "23   0.004100   -37.0       2.816573       2.634118  33.426823   40.038023   \n",
       "24   0.010200   -37.8       2.849849       2.540613  33.701728   40.191410   \n",
       "25   0.018200   -36.2       2.792812       2.541617  32.813255   40.061666   \n",
       "26   0.001200   -85.0       2.712397       2.262254  33.178027   38.589899   \n",
       "27   0.005080  -133.0       2.652343       2.235814  32.715828   40.554608   \n",
       "28   0.000119   -68.8       2.713491       2.349346  34.008461   39.370964   \n",
       "29   0.000119   -79.5       2.700877       2.305159  34.033689   39.358350   \n",
       "30   0.000442  -139.0       2.638489       2.065164  34.079702   38.391946   \n",
       "\n",
       "    log_luminosity  count  is_new_candidate  \n",
       "0        42.936302    287              True  \n",
       "1        41.921701   1000             False  \n",
       "2        42.842131    441              True  \n",
       "3        42.074499    998             False  \n",
       "4        40.824502    999             False  \n",
       "5        42.627843    989             False  \n",
       "6        41.517636    662             False  \n",
       "7        40.869004    795             False  \n",
       "8        42.407660    873             False  \n",
       "9        42.762846    978             False  \n",
       "10       37.431188   1000             False  \n",
       "11       41.292706    637             False  \n",
       "12       39.147168    603             False  \n",
       "13       42.175333    982             False  \n",
       "14       42.165654   1000             False  \n",
       "15       38.027190   1000             False  \n",
       "16       38.016559   1000             False  \n",
       "17       41.558863   1000             False  \n",
       "18       42.442111    903             False  \n",
       "19       42.410885    298             False  \n",
       "20       42.426495    562             False  \n",
       "21       42.275895    323              True  \n",
       "22       41.826064    780             False  \n",
       "23       42.362821    373             False  \n",
       "24       42.329849    533             False  \n",
       "25       41.813023    169             False  \n",
       "26       41.058398    562             False  \n",
       "27       42.408981    163             False  \n",
       "28       41.798246    123             False  \n",
       "29       41.785632    100             False  \n",
       "30       41.062479    223             False  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties = [\n",
    "    \"ra\",\n",
    "    \"dec\",\n",
    "    \"in_duration\",\n",
    "    \"log_dm_exc_ne2001\",\n",
    "    \"scat_time\",\n",
    "    \"sp_run\",\n",
    "    \"log_peak_freq\",\n",
    "    \"log_fre_width\",\n",
    "    \"log_T_B\",\n",
    "    \"log_energy\",\n",
    "    \"log_luminosity\",\n",
    "]\n",
    "for index, row in predictions_df.iterrows():\n",
    "    tns_name = row[\"tns_name\"]\n",
    "    sub_num = row[\"sub_num\"]\n",
    "\n",
    "    # Get FRB data from the original df\n",
    "    original_df_subb_rows = df[df[\"tns_name\"] == tns_name]\n",
    "    original_df_subb_row = original_df_subb_rows[\n",
    "        original_df_subb_rows[\"sub_num\"] == sub_num\n",
    "    ]\n",
    "\n",
    "    properties_dict = {}\n",
    "    for property in properties:\n",
    "        properties_dict[property] = original_df_subb_row[property].values[0]\n",
    "\n",
    "    # Modify the row in predictions_df with the new properties as separate columns\n",
    "    for key, value in properties_dict.items():\n",
    "        predictions_df.loc[index, key] = value\n",
    "    # Move sub_num and count to the end of the columns\n",
    "predictions_df = predictions_df[\n",
    "    [\"tns_name\", \"sub_num\"] + properties + [\"count\", \"is_new_candidate\"]\n",
    "]\n",
    "predictions_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
